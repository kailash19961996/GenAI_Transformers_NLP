{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import sentencepiece as spm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_df = pd.read_json('train_data.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One day, a little girl named Lily found a need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there was a little car named...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One day, a little fish named Fin was swimming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, in a land full of trees, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, there was a little girl name...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  One day, a little girl named Lily found a need...\n",
       "1  Once upon a time, there was a little car named...\n",
       "2  One day, a little fish named Fin was swimming ...\n",
       "3  Once upon a time, in a land full of trees, the...\n",
       "4  Once upon a time, there was a little girl name..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One day, a little girl named Lily found a need...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  One day, a little girl named Lily found a need..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = full_train_df[:1]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inputs_and_labels(df, sp):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        tokens = text.split()  # Tokenize the text by splitting on whitespace\n",
    "        \n",
    "        input_sequence = tokens # Generate input by adding start of sentence token at the beginning\n",
    "        label_sequence = tokens # Generate label by adding end of sentence token at the end\n",
    "\n",
    "        inputs.append(input_sequence)\n",
    "        labels.append(label_sequence)\n",
    "\n",
    "    # Convert input and label sequences to strings\n",
    "    input_strings = [' '.join(sequence) for sequence in inputs]\n",
    "    label_strings = [' '.join(sequence) for sequence in labels]\n",
    "\n",
    "    # Tokenize input strings and add <sos> token at the beginning\n",
    "    tokenized_inputs = []\n",
    "    input_ids = []\n",
    "    for sequence in input_strings:\n",
    "        tokenized_sequence = sp.encode_as_pieces(sequence)\n",
    "        tokenized_sequence = ['<sos>'] + tokenized_sequence  # Add <sos> token manually\n",
    "        input_ids.append([sp.piece_to_id('<sos>')] + sp.encode_as_ids(sequence))  # Get token IDs\n",
    "        tokenized_inputs.append(tokenized_sequence)\n",
    "\n",
    "    # Tokenize label strings and add </sos> token at the end\n",
    "    tokenized_labels = []\n",
    "    label_ids = []\n",
    "    for sequence in label_strings:\n",
    "        tokenized_sequence = sp.encode_as_pieces(sequence)\n",
    "        tokenized_sequence.append('</sos>')  # Add </sos> token manually at the end\n",
    "        label_ids.append(sp.encode_as_ids(sequence) + [sp.piece_to_id('</sos>')])  # Get token IDs\n",
    "        tokenized_labels.append(tokenized_sequence)\n",
    "\n",
    "    # Print tokenized input and label sequences\n",
    "    for i in range(len(inputs)):\n",
    "        print(\"Input Text:\", input_strings[i])\n",
    "        print(\"Tokenized Input:\", tokenized_inputs[i])\n",
    "        print(\"Input IDs:\", input_ids[i])\n",
    "        print(\"Label Text:\", label_strings[i])\n",
    "        print(\"Tokenized Label:\", tokenized_labels[i])\n",
    "        print(\"Label IDs:\", label_ids[i])\n",
    "\n",
    "    return input_ids, label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt. Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\" Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n",
      "Tokenized Input: ['<sos>', '▁one', '▁day', ',', '▁a', '▁little', '▁girl', '▁name', 'd', '▁lily', '▁found', '▁a', '▁needle', '▁in', '▁her', '▁room', '.', '▁she', '▁knew', '▁it', '▁was', '▁difficult', '▁to', '▁play', '▁with', '▁it', '▁', 'because', '▁it', '▁was', '▁sharp', '.', '▁lily', '▁wanted', '▁to', '▁share', '▁the', '▁needle', '▁with', '▁her', '▁mom', ',', '▁so', '▁she', '▁could', '▁sew', '▁a', '▁button', '▁on', '▁her', '▁shirt', '.', '▁lily', '▁went', '▁to', '▁her', '▁mom', '▁and', '▁said', ',', '▁\"', 'mom', ',', '▁i', '▁found', '▁this', '▁needle', '.', '▁can', '▁you', '▁share', '▁it', '▁with', '▁me', '▁and', '▁sew', '▁my', '▁shirt', '?\"', '▁her', '▁mom', '▁smiled', '▁and', '▁said', ',', '▁\"', 'yes', ',', '▁lily', ',', '▁we', '▁can', '▁share', '▁the', '▁needle', '▁and', '▁fix', '▁your', '▁shirt', '.\"', '▁together', ',', '▁they', '▁shared', '▁the', '▁needle', '▁and', '▁sew', 'ed', '▁the', '▁button', '▁on', '▁lily', \"'\", 's', '▁shirt', '.', '▁it', '▁was', '▁not', '▁difficult', '▁for', '▁them', '▁', 'because', '▁they', '▁were', '▁sharing', '▁and', '▁helping', '▁each', '▁other', '.', '▁after', '▁they', '▁finished', ',', '▁lily', '▁thank', 'ed', '▁her', '▁mom', '▁for', '▁sharing', '▁the', '▁needle', '▁and', '▁fix', 'ing', '▁her', '▁shirt', '.', '▁they', '▁both', '▁felt', '▁happy', '▁', 'because', '▁they', '▁had', '▁shared', '▁and', '▁worked', '▁together', '.']\n",
      "Input IDs: [0, 38, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59, 7, 259, 4, 1614, 24, 14, 43, 6, 23, 11, 94, 2599, 8, 1293, 32, 14, 802, 3, 31, 68, 7, 14, 43, 5, 18, 6, 16, 749, 6, 49, 119, 149, 1614, 3, 66, 25, 259, 12, 24, 145, 5, 2599, 140, 802, 82, 14, 43, 76, 5, 18, 6, 16, 257, 6, 31, 6, 96, 66, 259, 4, 1614, 5, 524, 129, 802, 46, 104, 6, 13, 678, 4, 1614, 5, 2599, 20, 4, 1293, 32, 31, 17, 15, 802, 3, 12, 9, 60, 1455, 36, 64, 19, 230, 13, 50, 1901, 5, 820, 183, 125, 3, 167, 13, 589, 6, 31, 146, 20, 14, 43, 36, 1901, 4, 1614, 5, 524, 55, 14, 802, 3, 13, 261, 90, 44, 19, 230, 13, 29, 678, 5, 521, 104, 3]\n",
      "Label Text: One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt. Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\" Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n",
      "Tokenized Label: ['▁one', '▁day', ',', '▁a', '▁little', '▁girl', '▁name', 'd', '▁lily', '▁found', '▁a', '▁needle', '▁in', '▁her', '▁room', '.', '▁she', '▁knew', '▁it', '▁was', '▁difficult', '▁to', '▁play', '▁with', '▁it', '▁', 'because', '▁it', '▁was', '▁sharp', '.', '▁lily', '▁wanted', '▁to', '▁share', '▁the', '▁needle', '▁with', '▁her', '▁mom', ',', '▁so', '▁she', '▁could', '▁sew', '▁a', '▁button', '▁on', '▁her', '▁shirt', '.', '▁lily', '▁went', '▁to', '▁her', '▁mom', '▁and', '▁said', ',', '▁\"', 'mom', ',', '▁i', '▁found', '▁this', '▁needle', '.', '▁can', '▁you', '▁share', '▁it', '▁with', '▁me', '▁and', '▁sew', '▁my', '▁shirt', '?\"', '▁her', '▁mom', '▁smiled', '▁and', '▁said', ',', '▁\"', 'yes', ',', '▁lily', ',', '▁we', '▁can', '▁share', '▁the', '▁needle', '▁and', '▁fix', '▁your', '▁shirt', '.\"', '▁together', ',', '▁they', '▁shared', '▁the', '▁needle', '▁and', '▁sew', 'ed', '▁the', '▁button', '▁on', '▁lily', \"'\", 's', '▁shirt', '.', '▁it', '▁was', '▁not', '▁difficult', '▁for', '▁them', '▁', 'because', '▁they', '▁were', '▁sharing', '▁and', '▁helping', '▁each', '▁other', '.', '▁after', '▁they', '▁finished', ',', '▁lily', '▁thank', 'ed', '▁her', '▁mom', '▁for', '▁sharing', '▁the', '▁needle', '▁and', '▁fix', 'ing', '▁her', '▁shirt', '.', '▁they', '▁both', '▁felt', '▁happy', '▁', 'because', '▁they', '▁had', '▁shared', '▁and', '▁worked', '▁together', '.', '</sos>']\n",
      "Label IDs: [38, 28, 6, 8, 37, 53, 86, 34, 31, 119, 8, 1614, 21, 14, 198, 3, 11, 185, 12, 9, 1455, 7, 54, 24, 12, 19, 230, 12, 9, 1316, 3, 31, 59, 7, 259, 4, 1614, 24, 14, 43, 6, 23, 11, 94, 2599, 8, 1293, 32, 14, 802, 3, 31, 68, 7, 14, 43, 5, 18, 6, 16, 749, 6, 49, 119, 149, 1614, 3, 66, 25, 259, 12, 24, 145, 5, 2599, 140, 802, 82, 14, 43, 76, 5, 18, 6, 16, 257, 6, 31, 6, 96, 66, 259, 4, 1614, 5, 524, 129, 802, 46, 104, 6, 13, 678, 4, 1614, 5, 2599, 20, 4, 1293, 32, 31, 17, 15, 802, 3, 12, 9, 60, 1455, 36, 64, 19, 230, 13, 50, 1901, 5, 820, 183, 125, 3, 167, 13, 589, 6, 31, 146, 20, 14, 43, 36, 1901, 4, 1614, 5, 524, 55, 14, 802, 3, 13, 261, 90, 44, 19, 230, 13, 29, 678, 5, 521, 104, 3, 0]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('small_m.model')\n",
    "\n",
    "source_ids, target_ids = generate_inputs_and_labels(train_df, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, source_ids, target_ids):\n",
    "        self.source_ids = source_ids\n",
    "        self.target_ids = target_ids\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.source_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source_sequence = self.source_ids[idx]\n",
    "        target_sequence = self.target_ids[idx]\n",
    "        return source_sequence, target_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Define a custom dataset class to process data from DataFrame\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DataFrameDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sos_token = '<sos>'\n",
    "        self.eos_token = '</sos>'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        input_seq, target_seq = self.process_text(row['text'])\n",
    "        return input_seq, target_seq\n",
    "\n",
    "    def process_text(self, text):\n",
    "        tokens = self.tokenizer.encode_as_pieces(text)\n",
    "        input_seq = [self.sos_token] + tokens[:-1]\n",
    "        target_seq = tokens\n",
    "        return input_seq, target_seq\n",
    "\n",
    "    def get_tokens_for_row(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        input_seq, target_seq = self.process_text(row['text'])\n",
    "        return input_seq, target_seq\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_seqs, tgt_seqs = zip(*batch)\n",
    "    src_seqs = pad_sequence(src_seqs, padding_value=0, batch_first=True)\n",
    "    tgt_seqs = pad_sequence(tgt_seqs, padding_value=0, batch_first=True)\n",
    "    return src_seqs, tgt_seqs\n",
    "\n",
    "# Load tokenizer and embedding model\n",
    "def load_tokenizer():\n",
    "    # Load your tokenizer model here\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load('small_m.model')\n",
    "    return sp\n",
    "\n",
    "def load_embedding_model():\n",
    "    # Load your embedding model here\n",
    "    return Word2Vec.load(\"small_word2vec.model\")\n",
    "\n",
    "# Define embedding layer\n",
    "class PretrainedEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, embedding_model):\n",
    "        super(PretrainedEmbeddingLayer, self).__init__()\n",
    "        self.embedding_model = embedding_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for idx in x:\n",
    "            token = tokenizer.index_word.get(idx.item(), '')  \n",
    "            if token in self.embedding_model:\n",
    "                embeddings.append(torch.tensor(self.embedding_model[token]))\n",
    "            else:\n",
    "                embeddings.append(torch.zeros(self.embedding_model.vector_size))\n",
    "        return torch.stack(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n"
     ]
    }
   ],
   "source": [
    "# Load and process data from DataFrame\n",
    "df = train_df[:1] # Load your DataFrame here\n",
    "\n",
    "# Load tokenizer and embedding model\n",
    "tokenizer = load_tokenizer()\n",
    "embedding_model = load_embedding_model()\n",
    "\n",
    "# Define embedding layer using pre-trained embeddings\n",
    "embedding_layer = PretrainedEmbeddingLayer(embedding_model)\n",
    "\n",
    "text = df['text'][0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Tokens: ['<sos>', '▁one', '▁day', ',', '▁a', '▁little', '▁girl', '▁name', 'd', '▁lily', '▁found', '▁a', '▁needle', '▁in', '▁her', '▁room', '.', '▁she', '▁knew', '▁it', '▁was', '▁difficult', '▁to', '▁play', '▁with', '▁it', '▁', 'because', '▁it', '▁was', '▁sharp', '.', '▁lily', '▁wanted', '▁to', '▁share', '▁the', '▁needle', '▁with', '▁her', '▁mom', ',', '▁so', '▁she', '▁could', '▁sew', '▁a', '▁button', '▁on', '▁her', '▁shirt', '.', '\\n\\n', 'lily', '▁went', '▁to', '▁her', '▁mom', '▁and', '▁said', ',', '▁\"', 'mom', ',', '▁i', '▁found', '▁this', '▁needle', '.', '▁can', '▁you', '▁share', '▁it', '▁with', '▁me', '▁and', '▁sew', '▁my', '▁shirt', '?\"', '▁her', '▁mom', '▁smiled', '▁and', '▁said', ',', '▁\"', 'yes', ',', '▁lily', ',', '▁we', '▁can', '▁share', '▁the', '▁needle', '▁and', '▁fix', '▁your', '▁shirt', '.\"', '\\n\\n', 'to', 'ge', 'ther', ',', '▁they', '▁shared', '▁the', '▁needle', '▁and', '▁sew', 'ed', '▁the', '▁button', '▁on', '▁lily', \"'\", 's', '▁shirt', '.', '▁it', '▁was', '▁not', '▁difficult', '▁for', '▁them', '▁', 'because', '▁they', '▁were', '▁sharing', '▁and', '▁helping', '▁each', '▁other', '.', '▁after', '▁they', '▁finished', ',', '▁lily', '▁thank', 'ed', '▁her', '▁mom', '▁for', '▁sharing', '▁the', '▁needle', '▁and', '▁fix', 'ing', '▁her', '▁shirt', '.', '▁they', '▁both', '▁felt', '▁happy', '▁', 'because', '▁they', '▁had', '▁shared', '▁and', '▁worked', '▁together']\n",
      "Target Tokens: ['▁one', '▁day', ',', '▁a', '▁little', '▁girl', '▁name', 'd', '▁lily', '▁found', '▁a', '▁needle', '▁in', '▁her', '▁room', '.', '▁she', '▁knew', '▁it', '▁was', '▁difficult', '▁to', '▁play', '▁with', '▁it', '▁', 'because', '▁it', '▁was', '▁sharp', '.', '▁lily', '▁wanted', '▁to', '▁share', '▁the', '▁needle', '▁with', '▁her', '▁mom', ',', '▁so', '▁she', '▁could', '▁sew', '▁a', '▁button', '▁on', '▁her', '▁shirt', '.', '\\n\\n', 'lily', '▁went', '▁to', '▁her', '▁mom', '▁and', '▁said', ',', '▁\"', 'mom', ',', '▁i', '▁found', '▁this', '▁needle', '.', '▁can', '▁you', '▁share', '▁it', '▁with', '▁me', '▁and', '▁sew', '▁my', '▁shirt', '?\"', '▁her', '▁mom', '▁smiled', '▁and', '▁said', ',', '▁\"', 'yes', ',', '▁lily', ',', '▁we', '▁can', '▁share', '▁the', '▁needle', '▁and', '▁fix', '▁your', '▁shirt', '.\"', '\\n\\n', 'to', 'ge', 'ther', ',', '▁they', '▁shared', '▁the', '▁needle', '▁and', '▁sew', 'ed', '▁the', '▁button', '▁on', '▁lily', \"'\", 's', '▁shirt', '.', '▁it', '▁was', '▁not', '▁difficult', '▁for', '▁them', '▁', 'because', '▁they', '▁were', '▁sharing', '▁and', '▁helping', '▁each', '▁other', '.', '▁after', '▁they', '▁finished', ',', '▁lily', '▁thank', 'ed', '▁her', '▁mom', '▁for', '▁sharing', '▁the', '▁needle', '▁and', '▁fix', 'ing', '▁her', '▁shirt', '.', '▁they', '▁both', '▁felt', '▁happy', '▁', 'because', '▁they', '▁had', '▁shared', '▁and', '▁worked', '▁together', '.']\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of DataFrameDataset\n",
    "dataset = DataFrameDataset(df, tokenizer)\n",
    "\n",
    "# Choose an index to print tokens for\n",
    "index = 0  # Change this to any index you want to inspect\n",
    "\n",
    "# Get the tokens for the chosen index\n",
    "source_tokens, target_tokens = dataset.get_tokens_for_row(index)\n",
    "\n",
    "# Print the tokens\n",
    "print(\"Source Tokens:\", source_tokens)\n",
    "print(\"Target Tokens:\", target_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to \"girl\": [('▁boy', 0.9662375450134277), ('▁name', 0.863869845867157), ('▁years', 0.8347034454345703), ('▁old', 0.8179454207420349), ('▁three', 0.8172581195831299), ('▁who', 0.8083735108375549), ('▁live', 0.7911251187324524), ('▁little', 0.7740851640701294), ('▁nosy', 0.7686184048652649), ('d', 0.7683912515640259)]\n"
     ]
    }
   ],
   "source": [
    "# Define a function to get embeddings and similar words\n",
    "def get_embedding_and_similar_words(word):\n",
    "    # Get embeddings of the word\n",
    "    encoded_word = tokenizer.encode_as_pieces(word)\n",
    "    word_str = ''.join(encoded_word)\n",
    "    \n",
    "    if word_str in embedding_model.wv.key_to_index:\n",
    "        # Get embedding\n",
    "        embedding = embedding_model.wv[word_str]\n",
    "        #print(f'Embedding for \"{word}\": {embedding}')\n",
    "\n",
    "        # Get similar words\n",
    "        similar_words = embedding_model.wv.most_similar(word_str)\n",
    "        print(f'Similar words to \"{word}\": {similar_words}')\n",
    "    else:\n",
    "        print(f'Word \"{word}\" not found in the embedding model.')\n",
    "\n",
    "\n",
    "# Example usage\n",
    "word_to_check = 'girl'\n",
    "get_embedding_and_similar_words(word_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.max_seq_length = max_seq_length\n",
    "        pe = self._init_pe(max_seq_length, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_length = x.size(1)\n",
    "        pe = self.pe[:seq_length, :] if seq_length <= self.max_seq_length else self._init_pe(seq_length, self.d_model)\n",
    "        return x + pe\n",
    "\n",
    "    def _init_pe(self, seq_length, d_model):\n",
    "        pe = torch.zeros(seq_length, d_model)\n",
    "        position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        return pe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, decoder_input, decoder_mask):\n",
    "        attn_output = self.self_attn(decoder_input, decoder_input, decoder_input, decoder_mask)\n",
    "        x = self.norm1(decoder_input + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, tgt):\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return tgt_mask\n",
    "\n",
    "    def forward(self, decoder_input):\n",
    "        decoder_mask = self.generate_mask(decoder_input)\n",
    "        decoder_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(decoder_input)))\n",
    "\n",
    "        dec_output = decoder_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, decoder_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "#transformer = Transformer(src_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "#transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Initialize the model\n",
    "transformer = Transformer(tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (decoder_embedding): Embedding(5000, 512)\n",
      "  (positional_encoding): PositionalEncoding()\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0-5): 6 x DecoderLayer(\n",
      "      (self_attn): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (feed_forward): PositionWiseFeedForward(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=512, out_features=5000, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data shape: torch.Size([1, 165])\n",
      "Target data shape: torch.Size([1, 165])\n",
      "tensor([[   0,   38,   28,    6,    8,   37,   53,   86,   34,   31,  119,    8,\n",
      "         1614,   21,   14,  198,    3,   11,  185,   12,    9, 1455,    7,   54,\n",
      "           24,   12,   19,  230,   12,    9, 1316,    3,   31,   59,    7,  259,\n",
      "            4, 1614,   24,   14,   43,    6,   23,   11,   94, 2599,    8, 1293,\n",
      "           32,   14,  802,    3,   31,   68,    7,   14,   43,    5,   18,    6,\n",
      "           16,  749,    6,   49,  119,  149, 1614,    3,   66,   25,  259,   12,\n",
      "           24,  145,    5, 2599,  140,  802,   82,   14,   43,   76,    5,   18,\n",
      "            6,   16,  257,    6,   31,    6,   96,   66,  259,    4, 1614,    5,\n",
      "          524,  129,  802,   46,  104,    6,   13,  678,    4, 1614,    5, 2599,\n",
      "           20,    4, 1293,   32,   31,   17,   15,  802,    3,   12,    9,   60,\n",
      "         1455,   36,   64,   19,  230,   13,   50, 1901,    5,  820,  183,  125,\n",
      "            3,  167,   13,  589,    6,   31,  146,   20,   14,   43,   36, 1901,\n",
      "            4, 1614,    5,  524,   55,   14,  802,    3,   13,  261,   90,   44,\n",
      "           19,  230,   13,   29,  678,    5,  521,  104,    3]])\n",
      "tensor([[  38,   28,    6,    8,   37,   53,   86,   34,   31,  119,    8, 1614,\n",
      "           21,   14,  198,    3,   11,  185,   12,    9, 1455,    7,   54,   24,\n",
      "           12,   19,  230,   12,    9, 1316,    3,   31,   59,    7,  259,    4,\n",
      "         1614,   24,   14,   43,    6,   23,   11,   94, 2599,    8, 1293,   32,\n",
      "           14,  802,    3,   31,   68,    7,   14,   43,    5,   18,    6,   16,\n",
      "          749,    6,   49,  119,  149, 1614,    3,   66,   25,  259,   12,   24,\n",
      "          145,    5, 2599,  140,  802,   82,   14,   43,   76,    5,   18,    6,\n",
      "           16,  257,    6,   31,    6,   96,   66,  259,    4, 1614,    5,  524,\n",
      "          129,  802,   46,  104,    6,   13,  678,    4, 1614,    5, 2599,   20,\n",
      "            4, 1293,   32,   31,   17,   15,  802,    3,   12,    9,   60, 1455,\n",
      "           36,   64,   19,  230,   13,   50, 1901,    5,  820,  183,  125,    3,\n",
      "          167,   13,  589,    6,   31,  146,   20,   14,   43,   36, 1901,    4,\n",
      "         1614,    5,  524,   55,   14,  802,    3,   13,  261,   90,   44,   19,\n",
      "          230,   13,   29,  678,    5,  521,  104,    3,    0]])\n"
     ]
    }
   ],
   "source": [
    "# Convert lists to tensors\n",
    "source_data = torch.tensor(source_ids)\n",
    "target_data = torch.tensor(target_ids)\n",
    "\n",
    "# Print shapes\n",
    "print(\"Source data shape:\", source_data.shape)\n",
    "print(\"Target data shape:\", target_data.shape)\n",
    "\n",
    "print(source_data)\n",
    "print(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.684040069580078\n",
      "Epoch: 2, Loss: 6.700284004211426\n",
      "Epoch: 3, Loss: 5.907226085662842\n",
      "Epoch: 4, Loss: 4.79470682144165\n",
      "Epoch: 5, Loss: 4.811295509338379\n",
      "Epoch: 6, Loss: 4.1880879402160645\n",
      "Epoch: 7, Loss: 3.098968029022217\n",
      "Epoch: 8, Loss: 2.5268588066101074\n",
      "Epoch: 9, Loss: 2.1185877323150635\n",
      "Epoch: 10, Loss: 1.527596354484558\n",
      "Epoch: 11, Loss: 1.1251120567321777\n",
      "Epoch: 12, Loss: 0.854336142539978\n",
      "Epoch: 13, Loss: 0.6587713956832886\n",
      "Epoch: 14, Loss: 0.5733023881912231\n",
      "Epoch: 15, Loss: 0.4774659276008606\n",
      "Epoch: 16, Loss: 0.4077913761138916\n",
      "Epoch: 17, Loss: 0.33671510219573975\n",
      "Epoch: 18, Loss: 0.2796070873737335\n",
      "Epoch: 19, Loss: 0.2266368418931961\n",
      "Epoch: 20, Loss: 0.19065441191196442\n",
      "Epoch: 21, Loss: 0.15619532763957977\n",
      "Epoch: 22, Loss: 0.12294583022594452\n",
      "Epoch: 23, Loss: 0.10402785241603851\n",
      "Epoch: 24, Loss: 0.08071884512901306\n",
      "Epoch: 25, Loss: 0.06430551409721375\n",
      "Epoch: 26, Loss: 0.05896078050136566\n",
      "Epoch: 27, Loss: 0.042945388704538345\n",
      "Epoch: 28, Loss: 0.036182891577482224\n",
      "Epoch: 29, Loss: 0.02513916976749897\n",
      "Epoch: 30, Loss: 0.02218657359480858\n",
      "Epoch: 31, Loss: 0.016157766804099083\n",
      "Epoch: 32, Loss: 0.01308424025774002\n",
      "Epoch: 33, Loss: 0.014522673562169075\n",
      "Epoch: 34, Loss: 0.00918592233210802\n",
      "Epoch: 35, Loss: 0.009722191840410233\n",
      "Epoch: 36, Loss: 0.007494571153074503\n",
      "Epoch: 37, Loss: 0.005620291456580162\n",
      "Epoch: 38, Loss: 0.004963901359587908\n",
      "Epoch: 39, Loss: 0.004915353376418352\n",
      "Epoch: 40, Loss: 0.0053937481716275215\n",
      "Epoch: 41, Loss: 0.004365295171737671\n",
      "Epoch: 42, Loss: 0.0040892865508794785\n",
      "Epoch: 43, Loss: 0.004839802160859108\n",
      "Epoch: 44, Loss: 0.0035539164673537016\n",
      "Epoch: 45, Loss: 0.013331722468137741\n",
      "Epoch: 46, Loss: 0.003498577745631337\n",
      "Epoch: 47, Loss: 0.05136942118406296\n",
      "Epoch: 48, Loss: 0.05982273444533348\n",
      "Epoch: 49, Loss: 0.004729564301669598\n",
      "Epoch: 50, Loss: 0.12362141907215118\n",
      "Epoch: 51, Loss: 0.00661609647795558\n",
      "Epoch: 52, Loss: 0.0216910969465971\n",
      "Epoch: 53, Loss: 0.02613641880452633\n",
      "Epoch: 54, Loss: 0.006379201076924801\n",
      "Epoch: 55, Loss: 0.01253954041749239\n",
      "Epoch: 56, Loss: 0.003307900158688426\n",
      "Epoch: 57, Loss: 0.002813921310007572\n",
      "Epoch: 58, Loss: 0.0030703141819685698\n",
      "Epoch: 59, Loss: 0.0046511320397257805\n",
      "Epoch: 60, Loss: 0.003846146399155259\n",
      "Epoch: 61, Loss: 0.003082996467128396\n",
      "Epoch: 62, Loss: 0.004629556555300951\n",
      "Epoch: 63, Loss: 0.003930945415049791\n",
      "Epoch: 64, Loss: 0.0032855337485671043\n",
      "Epoch: 65, Loss: 0.002565606962889433\n",
      "Epoch: 66, Loss: 0.0034898279700428247\n",
      "Epoch: 67, Loss: 0.002058940241113305\n",
      "Epoch: 68, Loss: 0.001987744588404894\n",
      "Epoch: 69, Loss: 0.0019301798893138766\n",
      "Epoch: 70, Loss: 0.0018420954002067447\n",
      "Epoch: 71, Loss: 0.0017043428961187601\n",
      "Epoch: 72, Loss: 0.0016423693159595132\n",
      "Epoch: 73, Loss: 0.0015913498355075717\n",
      "Epoch: 74, Loss: 0.0015170533442869782\n",
      "Epoch: 75, Loss: 0.0014460341772064567\n",
      "Epoch: 76, Loss: 0.0014565029414370656\n",
      "Epoch: 77, Loss: 0.0014241019962355494\n",
      "Epoch: 78, Loss: 0.0014185179024934769\n",
      "Epoch: 79, Loss: 0.0013426203513517976\n",
      "Epoch: 80, Loss: 0.0012535039568319917\n",
      "Epoch: 81, Loss: 0.0013597655342891812\n",
      "Epoch: 82, Loss: 0.0011711438419297338\n",
      "Epoch: 83, Loss: 0.0011757101165130734\n",
      "Epoch: 84, Loss: 0.0011302938219159842\n",
      "Epoch: 85, Loss: 0.0011648926883935928\n",
      "Epoch: 86, Loss: 0.0011539686238393188\n",
      "Epoch: 87, Loss: 0.0010520964860916138\n",
      "Epoch: 88, Loss: 0.0010633160127326846\n",
      "Epoch: 89, Loss: 0.0010453216964378953\n",
      "Epoch: 90, Loss: 0.0010208915919065475\n",
      "Epoch: 91, Loss: 0.0009770657634362578\n",
      "Epoch: 92, Loss: 0.0009595442097634077\n",
      "Epoch: 93, Loss: 0.0008977995021268725\n",
      "Epoch: 94, Loss: 0.0009347169543616474\n",
      "Epoch: 95, Loss: 0.0008904662099666893\n",
      "Epoch: 96, Loss: 0.0008889834862202406\n",
      "Epoch: 97, Loss: 0.0008787027327343822\n",
      "Epoch: 98, Loss: 0.0008349863928742707\n",
      "Epoch: 99, Loss: 0.0008306814706884325\n",
      "Epoch: 100, Loss: 0.0008229316445067525\n",
      "Epoch: 101, Loss: 0.0007731376681476831\n",
      "Epoch: 102, Loss: 0.0007966288249008358\n",
      "Epoch: 103, Loss: 0.0007611764594912529\n",
      "Epoch: 104, Loss: 0.0007569552981294692\n",
      "Epoch: 105, Loss: 0.0007716523832641542\n",
      "Epoch: 106, Loss: 0.0007511699222959578\n",
      "Epoch: 107, Loss: 0.0006968396482989192\n",
      "Epoch: 108, Loss: 0.0007337766001001\n",
      "Epoch: 109, Loss: 0.000707247294485569\n",
      "Epoch: 110, Loss: 0.000694986607413739\n",
      "Epoch: 111, Loss: 0.0006940499297343194\n",
      "Epoch: 112, Loss: 0.0006847372860647738\n",
      "Epoch: 113, Loss: 0.0006619475316256285\n",
      "Epoch: 114, Loss: 0.0006523870397359133\n",
      "Epoch: 115, Loss: 0.0006633641896769404\n",
      "Epoch: 116, Loss: 0.0006416362593881786\n",
      "Epoch: 117, Loss: 0.0006404791492968798\n",
      "Epoch: 118, Loss: 0.0006446419283747673\n",
      "Epoch: 119, Loss: 0.000625528977252543\n",
      "Epoch: 120, Loss: 0.0006052267272025347\n",
      "Epoch: 121, Loss: 0.0006200519273988903\n",
      "Epoch: 122, Loss: 0.0006479349103756249\n",
      "Epoch: 123, Loss: 0.0006041585584171116\n",
      "Epoch: 124, Loss: 0.0005863207625225186\n",
      "Epoch: 125, Loss: 0.0005977250984869897\n",
      "Epoch: 126, Loss: 0.0005833230097778141\n",
      "Epoch: 127, Loss: 0.0005796347395516932\n",
      "Epoch: 128, Loss: 0.0005627440987154841\n",
      "Epoch: 129, Loss: 0.0005725174560211599\n",
      "Epoch: 130, Loss: 0.0005587079795077443\n",
      "Epoch: 131, Loss: 0.0005589485517702997\n",
      "Epoch: 132, Loss: 0.0005462666158564389\n",
      "Epoch: 133, Loss: 0.00053924391977489\n",
      "Epoch: 134, Loss: 0.0005285016377456486\n",
      "Epoch: 135, Loss: 0.0005308403051458299\n",
      "Epoch: 136, Loss: 0.0005177774000912905\n",
      "Epoch: 137, Loss: 0.0005215732962824404\n",
      "Epoch: 138, Loss: 0.0005073222564533353\n",
      "Epoch: 139, Loss: 0.0005094957887195051\n",
      "Epoch: 140, Loss: 0.000513000413775444\n",
      "Epoch: 141, Loss: 0.0004960136720910668\n",
      "Epoch: 142, Loss: 0.0004886791575700045\n",
      "Epoch: 143, Loss: 0.0004737638810183853\n",
      "Epoch: 144, Loss: 0.0004906874964945018\n",
      "Epoch: 145, Loss: 0.00048124295426532626\n",
      "Epoch: 146, Loss: 0.0004750119405798614\n",
      "Epoch: 147, Loss: 0.00046448377543129027\n",
      "Epoch: 148, Loss: 0.0004733063979074359\n",
      "Epoch: 149, Loss: 0.00045982838491909206\n",
      "Epoch: 150, Loss: 0.0004964015679433942\n",
      "Epoch: 151, Loss: 0.00045027866144664586\n",
      "Epoch: 152, Loss: 0.00045885227154940367\n",
      "Epoch: 153, Loss: 0.0004610598844010383\n",
      "Epoch: 154, Loss: 0.0004568267031572759\n",
      "Epoch: 155, Loss: 0.0004364040505606681\n",
      "Epoch: 156, Loss: 0.00045488192699849606\n",
      "Epoch: 157, Loss: 0.0004312835226301104\n",
      "Epoch: 158, Loss: 0.000432220142101869\n",
      "Epoch: 159, Loss: 0.00041692727245390415\n",
      "Epoch: 160, Loss: 0.00042095762910321355\n",
      "Epoch: 161, Loss: 0.0004213003849145025\n",
      "Epoch: 162, Loss: 0.00042203982593491673\n",
      "Epoch: 163, Loss: 0.00041455376776866615\n",
      "Epoch: 164, Loss: 0.0004189165774732828\n",
      "Epoch: 165, Loss: 0.0004080353246536106\n",
      "Epoch: 166, Loss: 0.00040408014319837093\n",
      "Epoch: 167, Loss: 0.00040085348882712424\n",
      "Epoch: 168, Loss: 0.00039841115358285606\n",
      "Epoch: 169, Loss: 0.0003859254065901041\n",
      "Epoch: 170, Loss: 0.00038985145511105657\n",
      "Epoch: 171, Loss: 0.0003862270968966186\n",
      "Epoch: 172, Loss: 0.0003785223525483161\n",
      "Epoch: 173, Loss: 0.0003706679563038051\n",
      "Epoch: 174, Loss: 0.0003750824835151434\n",
      "Epoch: 175, Loss: 0.0003811131464317441\n",
      "Epoch: 176, Loss: 0.0003661816008388996\n",
      "Epoch: 177, Loss: 0.00036966174957342446\n",
      "Epoch: 178, Loss: 0.0003615716123022139\n",
      "Epoch: 179, Loss: 0.00035984901478514075\n",
      "Epoch: 180, Loss: 0.00034814586979337037\n",
      "Epoch: 181, Loss: 0.0003700739180203527\n",
      "Epoch: 182, Loss: 0.00035299130831845105\n",
      "Epoch: 183, Loss: 0.0003444664762355387\n",
      "Epoch: 184, Loss: 0.0003362403658684343\n",
      "Epoch: 185, Loss: 0.00033719418570399284\n",
      "Epoch: 186, Loss: 0.00034039936144836247\n",
      "Epoch: 187, Loss: 0.00034080282784998417\n",
      "Epoch: 188, Loss: 0.000328191788867116\n",
      "Epoch: 189, Loss: 0.00032774085411801934\n",
      "Epoch: 190, Loss: 0.00031567399855703115\n",
      "Epoch: 191, Loss: 0.00032565309084020555\n",
      "Epoch: 192, Loss: 0.00032055459450930357\n",
      "Epoch: 193, Loss: 0.00031058062450028956\n",
      "Epoch: 194, Loss: 0.00031779511482454836\n",
      "Epoch: 195, Loss: 0.0003013296809513122\n",
      "Epoch: 196, Loss: 0.0003096300351899117\n",
      "Epoch: 197, Loss: 0.00030385376885533333\n",
      "Epoch: 198, Loss: 0.00030160535243339837\n",
      "Epoch: 199, Loss: 0.0003083057526964694\n",
      "Epoch: 200, Loss: 0.0003032210806850344\n",
      "Epoch: 201, Loss: 0.00029062555404379964\n",
      "Epoch: 202, Loss: 0.00029745171195827425\n",
      "Epoch: 203, Loss: 0.0002866048307623714\n",
      "Epoch: 204, Loss: 0.0002865828573703766\n",
      "Epoch: 205, Loss: 0.0002947215980384499\n",
      "Epoch: 206, Loss: 0.0002813667233567685\n",
      "Epoch: 207, Loss: 0.0002742334036156535\n",
      "Epoch: 208, Loss: 0.00028157985070720315\n",
      "Epoch: 209, Loss: 0.000283290195511654\n",
      "Epoch: 210, Loss: 0.0002747969701886177\n",
      "Epoch: 211, Loss: 0.00027524601318873465\n",
      "Epoch: 212, Loss: 0.000270046410150826\n",
      "Epoch: 213, Loss: 0.00026861598598770797\n",
      "Epoch: 214, Loss: 0.0002623253676574677\n",
      "Epoch: 215, Loss: 0.0002601937158033252\n",
      "Epoch: 216, Loss: 0.00026347525999881327\n",
      "Epoch: 217, Loss: 0.0002605315821710974\n",
      "Epoch: 218, Loss: 0.0002519350091461092\n",
      "Epoch: 219, Loss: 0.00025901835761033\n",
      "Epoch: 220, Loss: 0.00025701537379063666\n",
      "Epoch: 221, Loss: 0.00024532750830985606\n",
      "Epoch: 222, Loss: 0.00025217750226147473\n",
      "Epoch: 223, Loss: 0.0002496273082215339\n",
      "Epoch: 224, Loss: 0.0002418508956907317\n",
      "Epoch: 225, Loss: 0.00023495113418903202\n",
      "Epoch: 226, Loss: 0.00023925627465359867\n",
      "Epoch: 227, Loss: 0.00023646425688639283\n",
      "Epoch: 228, Loss: 0.00023605088063050061\n",
      "Epoch: 229, Loss: 0.00023506420257035643\n",
      "Epoch: 230, Loss: 0.00023401172074954957\n",
      "Epoch: 231, Loss: 0.00023264181800186634\n",
      "Epoch: 232, Loss: 0.00022632512263953686\n",
      "Epoch: 233, Loss: 0.00022376033302862197\n",
      "Epoch: 234, Loss: 0.00022245175205171108\n",
      "Epoch: 235, Loss: 0.00021975029085297137\n",
      "Epoch: 236, Loss: 0.0002213431871496141\n",
      "Epoch: 237, Loss: 0.00022032196284271777\n",
      "Epoch: 238, Loss: 0.0002175094123231247\n",
      "Epoch: 239, Loss: 0.0002164451580028981\n",
      "Epoch: 240, Loss: 0.00021687748085241765\n",
      "Epoch: 241, Loss: 0.0002111913199769333\n",
      "Epoch: 242, Loss: 0.00022345072648022324\n",
      "Epoch: 243, Loss: 0.0002081196871586144\n",
      "Epoch: 244, Loss: 0.00020282760669942945\n",
      "Epoch: 245, Loss: 0.00020505092106759548\n",
      "Epoch: 246, Loss: 0.00020185344328638166\n",
      "Epoch: 247, Loss: 0.00019873317796736956\n",
      "Epoch: 248, Loss: 0.00019509062985889614\n",
      "Epoch: 249, Loss: 0.0002026531146839261\n",
      "Epoch: 250, Loss: 0.00020148650219198316\n",
      "Epoch: 251, Loss: 0.00019622064428403974\n",
      "Epoch: 252, Loss: 0.00019250105833634734\n",
      "Epoch: 253, Loss: 0.00018980873574037105\n",
      "Epoch: 254, Loss: 0.0001936060143634677\n",
      "Epoch: 255, Loss: 0.00019354000687599182\n",
      "Epoch: 256, Loss: 0.00019368795619811863\n",
      "Epoch: 257, Loss: 0.00018461757281329483\n",
      "Epoch: 258, Loss: 0.00018336916400585324\n",
      "Epoch: 259, Loss: 0.0001844215003075078\n",
      "Epoch: 260, Loss: 0.00018469779752194881\n",
      "Epoch: 261, Loss: 0.00018335419008508325\n",
      "Epoch: 262, Loss: 0.00017956049123313278\n",
      "Epoch: 263, Loss: 0.00017742670024745166\n",
      "Epoch: 264, Loss: 0.0001734337129164487\n",
      "Epoch: 265, Loss: 0.00016897954628802836\n",
      "Epoch: 266, Loss: 0.00017526447481941432\n",
      "Epoch: 267, Loss: 0.00017354277952108532\n",
      "Epoch: 268, Loss: 0.00016893296560738236\n",
      "Epoch: 269, Loss: 0.0001707242481643334\n",
      "Epoch: 270, Loss: 0.00016569721628911793\n",
      "Epoch: 271, Loss: 0.0001622391282580793\n",
      "Epoch: 272, Loss: 0.0001689091295702383\n",
      "Epoch: 273, Loss: 0.00016211312322411686\n",
      "Epoch: 274, Loss: 0.0001653984363656491\n",
      "Epoch: 275, Loss: 0.00016082814545370638\n",
      "Epoch: 276, Loss: 0.00016040440823417157\n",
      "Epoch: 277, Loss: 0.00016060999769251794\n",
      "Epoch: 278, Loss: 0.00015335022180806845\n",
      "Epoch: 279, Loss: 0.00016012128617148846\n",
      "Epoch: 280, Loss: 0.00016078713815659285\n",
      "Epoch: 281, Loss: 0.0001495795586379245\n",
      "Epoch: 282, Loss: 0.00015259470092132688\n",
      "Epoch: 283, Loss: 0.00014777781325392425\n",
      "Epoch: 284, Loss: 0.00015101594908628613\n",
      "Epoch: 285, Loss: 0.0001434868754586205\n",
      "Epoch: 286, Loss: 0.000147078579175286\n",
      "Epoch: 287, Loss: 0.00014212583482731134\n",
      "Epoch: 288, Loss: 0.00014331034617498517\n",
      "Epoch: 289, Loss: 0.00013971724547445774\n",
      "Epoch: 290, Loss: 0.00013910287816543132\n",
      "Epoch: 291, Loss: 0.00013966122060082853\n",
      "Epoch: 292, Loss: 0.00013624703569803387\n",
      "Epoch: 293, Loss: 0.0001355150598101318\n",
      "Epoch: 294, Loss: 0.00013367054634727538\n",
      "Epoch: 295, Loss: 0.00013804982881993055\n",
      "Epoch: 296, Loss: 0.00013227644376456738\n",
      "Epoch: 297, Loss: 0.0001298003626288846\n",
      "Epoch: 298, Loss: 0.00013012073759455234\n",
      "Epoch: 299, Loss: 0.00012851814972236753\n",
      "Epoch: 300, Loss: 0.0001291380904149264\n",
      "Epoch: 301, Loss: 0.00013026973465457559\n",
      "Epoch: 302, Loss: 0.00012955372221767902\n",
      "Epoch: 303, Loss: 0.00012678263010457158\n",
      "Epoch: 304, Loss: 0.0001255536190001294\n",
      "Epoch: 305, Loss: 0.000121962686534971\n",
      "Epoch: 306, Loss: 0.00012110511306673288\n",
      "Epoch: 307, Loss: 0.00012238122872076929\n",
      "Epoch: 308, Loss: 0.00012146244262112305\n",
      "Epoch: 309, Loss: 0.0001170823525171727\n",
      "Epoch: 310, Loss: 0.00011666516365949064\n",
      "Epoch: 311, Loss: 0.00011726509546861053\n",
      "Epoch: 312, Loss: 0.0001140057502198033\n",
      "Epoch: 313, Loss: 0.00011449766316218302\n",
      "Epoch: 314, Loss: 0.00011568787158466876\n",
      "Epoch: 315, Loss: 0.00011243655171710998\n",
      "Epoch: 316, Loss: 0.00011103249562438577\n",
      "Epoch: 317, Loss: 0.00010864636715268716\n",
      "Epoch: 318, Loss: 0.00010910196579061449\n",
      "Epoch: 319, Loss: 0.000110362954728771\n",
      "Epoch: 320, Loss: 0.00010928810661425814\n",
      "Epoch: 321, Loss: 0.00010700136772356927\n",
      "Epoch: 322, Loss: 0.00010819493763847277\n",
      "Epoch: 323, Loss: 0.00010468965047039092\n",
      "Epoch: 324, Loss: 0.00010657751408871263\n",
      "Epoch: 325, Loss: 0.00010359932639403269\n",
      "Epoch: 326, Loss: 0.00010500558710191399\n",
      "Epoch: 327, Loss: 0.00010036286403192207\n",
      "Epoch: 328, Loss: 0.00010140074300579727\n",
      "Epoch: 329, Loss: 0.0001007377213682048\n",
      "Epoch: 330, Loss: 9.95278314803727e-05\n",
      "Epoch: 331, Loss: 0.00010091605508932844\n",
      "Epoch: 332, Loss: 0.00010069135896628723\n",
      "Epoch: 333, Loss: 9.807201422518119e-05\n",
      "Epoch: 334, Loss: 9.800564293982461e-05\n",
      "Epoch: 335, Loss: 9.445834439247847e-05\n",
      "Epoch: 336, Loss: 9.603000216884539e-05\n",
      "Epoch: 337, Loss: 9.3796246801503e-05\n",
      "Epoch: 338, Loss: 9.331583714811131e-05\n",
      "Epoch: 339, Loss: 9.379840776091442e-05\n",
      "Epoch: 340, Loss: 9.42503465921618e-05\n",
      "Epoch: 341, Loss: 8.998030534712598e-05\n",
      "Epoch: 342, Loss: 8.979658741736785e-05\n",
      "Epoch: 343, Loss: 8.983501902548596e-05\n",
      "Epoch: 344, Loss: 8.90704759513028e-05\n",
      "Epoch: 345, Loss: 8.752162102609873e-05\n",
      "Epoch: 346, Loss: 8.853692270349711e-05\n",
      "Epoch: 347, Loss: 8.544227603124455e-05\n",
      "Epoch: 348, Loss: 8.531136700185016e-05\n",
      "Epoch: 349, Loss: 8.431686728727072e-05\n",
      "Epoch: 350, Loss: 8.418252400588244e-05\n",
      "Epoch: 351, Loss: 8.30197604955174e-05\n",
      "Epoch: 352, Loss: 8.159445133060217e-05\n",
      "Epoch: 353, Loss: 8.223039912991226e-05\n",
      "Epoch: 354, Loss: 8.25186216388829e-05\n",
      "Epoch: 355, Loss: 8.125212480081245e-05\n",
      "Epoch: 356, Loss: 8.03028597147204e-05\n",
      "Epoch: 357, Loss: 8.031736069824547e-05\n",
      "Epoch: 358, Loss: 7.866680971346796e-05\n",
      "Epoch: 359, Loss: 7.678069232497364e-05\n",
      "Epoch: 360, Loss: 7.740868750261143e-05\n",
      "Epoch: 361, Loss: 7.686790195293725e-05\n",
      "Epoch: 362, Loss: 7.810886745573953e-05\n",
      "Epoch: 363, Loss: 7.454134174622595e-05\n",
      "Epoch: 364, Loss: 7.512934826081619e-05\n",
      "Epoch: 365, Loss: 7.400699541904032e-05\n",
      "Epoch: 366, Loss: 7.487708353437483e-05\n",
      "Epoch: 367, Loss: 7.227799505926669e-05\n",
      "Epoch: 368, Loss: 7.071022992022336e-05\n",
      "Epoch: 369, Loss: 7.196763181127608e-05\n",
      "Epoch: 370, Loss: 6.950006354600191e-05\n",
      "Epoch: 371, Loss: 7.146310235839337e-05\n",
      "Epoch: 372, Loss: 7.066149555612355e-05\n",
      "Epoch: 373, Loss: 6.882625893922523e-05\n",
      "Epoch: 374, Loss: 6.747076258761808e-05\n",
      "Epoch: 375, Loss: 7.735770486760885e-05\n",
      "Epoch: 376, Loss: 6.791992200305685e-05\n",
      "Epoch: 377, Loss: 6.872445374028757e-05\n",
      "Epoch: 378, Loss: 6.727666186634451e-05\n",
      "Epoch: 379, Loss: 6.941985338926315e-05\n",
      "Epoch: 380, Loss: 6.546392251038924e-05\n",
      "Epoch: 381, Loss: 6.536141881952062e-05\n",
      "Epoch: 382, Loss: 6.498845323221758e-05\n",
      "Epoch: 383, Loss: 6.59689394524321e-05\n",
      "Epoch: 384, Loss: 0.0001355273270746693\n",
      "Epoch: 385, Loss: 6.567205127794296e-05\n",
      "Epoch: 386, Loss: 0.04302150383591652\n",
      "Epoch: 387, Loss: 0.15806905925273895\n",
      "Epoch: 388, Loss: 0.024858133867383003\n",
      "Epoch: 389, Loss: 0.19115576148033142\n",
      "Epoch: 390, Loss: 0.46423640847206116\n",
      "Epoch: 391, Loss: 0.11940610408782959\n",
      "Epoch: 392, Loss: 0.166330948472023\n",
      "Epoch: 393, Loss: 0.0893121063709259\n",
      "Epoch: 394, Loss: 0.28248053789138794\n",
      "Epoch: 395, Loss: 0.3989250361919403\n",
      "Epoch: 396, Loss: 0.3074193000793457\n",
      "Epoch: 397, Loss: 0.13399547338485718\n",
      "Epoch: 398, Loss: 0.07238007336854935\n",
      "Epoch: 399, Loss: 0.11795191466808319\n",
      "Epoch: 400, Loss: 0.04627039656043053\n",
      "Epoch: 401, Loss: 0.06534453481435776\n",
      "Epoch: 402, Loss: 0.04649852588772774\n",
      "Epoch: 403, Loss: 0.014634687453508377\n",
      "Epoch: 404, Loss: 0.09019318968057632\n",
      "Epoch: 405, Loss: 0.016555484384298325\n",
      "Epoch: 406, Loss: 0.014516936615109444\n",
      "Epoch: 407, Loss: 0.03267216309905052\n",
      "Epoch: 408, Loss: 0.00884891115128994\n",
      "Epoch: 409, Loss: 0.06982067972421646\n",
      "Epoch: 410, Loss: 0.007645902223885059\n",
      "Epoch: 411, Loss: 0.09951131790876389\n",
      "Epoch: 412, Loss: 0.008045278489589691\n",
      "Epoch: 413, Loss: 0.008122539147734642\n",
      "Epoch: 414, Loss: 0.04396510869264603\n",
      "Epoch: 415, Loss: 0.031076742336153984\n",
      "Epoch: 416, Loss: 0.013490365818142891\n",
      "Epoch: 417, Loss: 0.08468864858150482\n",
      "Epoch: 418, Loss: 0.08376848697662354\n",
      "Epoch: 419, Loss: 0.026074254885315895\n",
      "Epoch: 420, Loss: 0.016564544290304184\n",
      "Epoch: 421, Loss: 0.11973822116851807\n",
      "Epoch: 422, Loss: 0.08510815352201462\n",
      "Epoch: 423, Loss: 0.06350874155759811\n",
      "Epoch: 424, Loss: 0.07505547255277634\n",
      "Epoch: 425, Loss: 0.02492748573422432\n",
      "Epoch: 426, Loss: 0.04871276393532753\n",
      "Epoch: 427, Loss: 0.053818050771951675\n",
      "Epoch: 428, Loss: 0.022130122408270836\n",
      "Epoch: 429, Loss: 0.04728122800588608\n",
      "Epoch: 430, Loss: 0.021705275401473045\n",
      "Epoch: 431, Loss: 0.010020957328379154\n",
      "Epoch: 432, Loss: 0.026590881869196892\n",
      "Epoch: 433, Loss: 0.006827943027019501\n",
      "Epoch: 434, Loss: 0.007577585522085428\n",
      "Epoch: 435, Loss: 0.01463423389941454\n",
      "Epoch: 436, Loss: 0.0040230159647762775\n",
      "Epoch: 437, Loss: 0.010862119495868683\n",
      "Epoch: 438, Loss: 0.008530573919415474\n",
      "Epoch: 439, Loss: 0.00554612884297967\n",
      "Epoch: 440, Loss: 0.002397073432803154\n",
      "Epoch: 441, Loss: 0.004250397905707359\n",
      "Epoch: 442, Loss: 0.0018615479348227382\n",
      "Epoch: 443, Loss: 0.0034229569137096405\n",
      "Epoch: 444, Loss: 0.00577105488628149\n",
      "Epoch: 445, Loss: 0.0017286611255258322\n",
      "Epoch: 446, Loss: 0.0010934440651908517\n",
      "Epoch: 447, Loss: 0.0009746287250891328\n",
      "Epoch: 448, Loss: 0.001550636487081647\n",
      "Epoch: 449, Loss: 0.0015874238451942801\n",
      "Epoch: 450, Loss: 0.0008047569426707923\n",
      "Epoch: 451, Loss: 0.0014727726811543107\n",
      "Epoch: 452, Loss: 0.0006255294429138303\n",
      "Epoch: 453, Loss: 0.0014815735630691051\n",
      "Epoch: 454, Loss: 0.0007128688157536089\n",
      "Epoch: 455, Loss: 0.0011707793455570936\n",
      "Epoch: 456, Loss: 0.0008594511891715229\n",
      "Epoch: 457, Loss: 0.0005231204559095204\n",
      "Epoch: 458, Loss: 0.0005982586299069226\n",
      "Epoch: 459, Loss: 0.0027662445791065693\n",
      "Epoch: 460, Loss: 0.0003281224344391376\n",
      "Epoch: 461, Loss: 0.0012640503700822592\n",
      "Epoch: 462, Loss: 0.004162331111729145\n",
      "Epoch: 463, Loss: 0.0005198558792471886\n",
      "Epoch: 464, Loss: 0.005330980289727449\n",
      "Epoch: 465, Loss: 0.00035255320835858583\n",
      "Epoch: 466, Loss: 0.019273849204182625\n",
      "Epoch: 467, Loss: 0.008148366585373878\n",
      "Epoch: 468, Loss: 0.028012052178382874\n",
      "Epoch: 469, Loss: 0.009465400129556656\n",
      "Epoch: 470, Loss: 0.0025422577746212482\n",
      "Epoch: 471, Loss: 0.0012347265146672726\n",
      "Epoch: 472, Loss: 0.0017780506750568748\n",
      "Epoch: 473, Loss: 0.00250163278542459\n",
      "Epoch: 474, Loss: 0.014160205610096455\n",
      "Epoch: 475, Loss: 0.06605780869722366\n",
      "Epoch: 476, Loss: 0.001161612686701119\n",
      "Epoch: 477, Loss: 0.0262126661837101\n",
      "Epoch: 478, Loss: 0.03115442954003811\n",
      "Epoch: 479, Loss: 0.043780963867902756\n",
      "Epoch: 480, Loss: 0.07236114889383316\n",
      "Epoch: 481, Loss: 0.1365921050310135\n",
      "Epoch: 482, Loss: 0.003407558659091592\n",
      "Epoch: 483, Loss: 0.0508401058614254\n",
      "Epoch: 484, Loss: 0.006114529445767403\n",
      "Epoch: 485, Loss: 0.11930543929338455\n",
      "Epoch: 486, Loss: 0.025619274005293846\n",
      "Epoch: 487, Loss: 0.03956190124154091\n",
      "Epoch: 488, Loss: 0.020375141873955727\n",
      "Epoch: 489, Loss: 0.010296192020177841\n",
      "Epoch: 490, Loss: 0.030630651861429214\n",
      "Epoch: 491, Loss: 0.030747992917895317\n",
      "Epoch: 492, Loss: 0.06368134915828705\n",
      "Epoch: 493, Loss: 0.0022670645266771317\n",
      "Epoch: 494, Loss: 0.011973227374255657\n",
      "Epoch: 495, Loss: 0.017522528767585754\n",
      "Epoch: 496, Loss: 0.06635807454586029\n",
      "Epoch: 497, Loss: 0.04134615883231163\n",
      "Epoch: 498, Loss: 0.03459104150533676\n",
      "Epoch: 499, Loss: 0.059752028435468674\n",
      "Epoch: 500, Loss: 0.011434439569711685\n",
      "Epoch: 501, Loss: 0.014595544897019863\n",
      "Epoch: 502, Loss: 0.0033234136644750834\n",
      "Epoch: 503, Loss: 0.029505476355552673\n",
      "Epoch: 504, Loss: 0.024045364931225777\n",
      "Epoch: 505, Loss: 0.15840359032154083\n",
      "Epoch: 506, Loss: 0.008419565856456757\n",
      "Epoch: 507, Loss: 0.08619027584791183\n",
      "Epoch: 508, Loss: 0.019100932404398918\n",
      "Epoch: 509, Loss: 0.014664111658930779\n",
      "Epoch: 510, Loss: 0.02531546540558338\n",
      "Epoch: 511, Loss: 0.0169445239007473\n",
      "Epoch: 512, Loss: 0.018218064680695534\n",
      "Epoch: 513, Loss: 0.014110572636127472\n",
      "Epoch: 514, Loss: 0.013698291033506393\n",
      "Epoch: 515, Loss: 0.0014063477283343673\n",
      "Epoch: 516, Loss: 0.005844259634613991\n",
      "Epoch: 517, Loss: 0.03532464802265167\n",
      "Epoch: 518, Loss: 0.04730600118637085\n",
      "Epoch: 519, Loss: 0.04943813756108284\n",
      "Epoch: 520, Loss: 0.1228344589471817\n",
      "Epoch: 521, Loss: 0.007794048637151718\n",
      "Epoch: 522, Loss: 0.3250585198402405\n",
      "Epoch: 523, Loss: 0.14930875599384308\n",
      "Epoch: 524, Loss: 0.14474256336688995\n",
      "Epoch: 525, Loss: 0.10386703908443451\n",
      "Epoch: 526, Loss: 0.06254830956459045\n",
      "Epoch: 527, Loss: 0.04476017877459526\n",
      "Epoch: 528, Loss: 0.011615225113928318\n",
      "Epoch: 529, Loss: 0.008017659187316895\n",
      "Epoch: 530, Loss: 0.02093610167503357\n",
      "Epoch: 531, Loss: 0.006406642496585846\n",
      "Epoch: 532, Loss: 0.018124256283044815\n",
      "Epoch: 533, Loss: 0.003001017030328512\n",
      "Epoch: 534, Loss: 0.004862016998231411\n",
      "Epoch: 535, Loss: 0.010989776812493801\n",
      "Epoch: 536, Loss: 0.006498620379716158\n",
      "Epoch: 537, Loss: 0.043876297771930695\n",
      "Epoch: 538, Loss: 0.010378066450357437\n",
      "Epoch: 539, Loss: 0.0977596789598465\n",
      "Epoch: 540, Loss: 0.01529653649777174\n",
      "Epoch: 541, Loss: 0.10320045799016953\n",
      "Epoch: 542, Loss: 0.17167387902736664\n",
      "Epoch: 543, Loss: 0.08114113658666611\n",
      "Epoch: 544, Loss: 0.03691922128200531\n",
      "Epoch: 545, Loss: 0.04046263173222542\n",
      "Epoch: 546, Loss: 0.06501688063144684\n",
      "Epoch: 547, Loss: 0.020801343023777008\n",
      "Epoch: 548, Loss: 0.019578320905566216\n",
      "Epoch: 549, Loss: 0.020786477252840996\n",
      "Epoch: 550, Loss: 0.018880143761634827\n",
      "Epoch: 551, Loss: 0.01592699997127056\n",
      "Epoch: 552, Loss: 0.013315265998244286\n",
      "Epoch: 553, Loss: 0.0162036269903183\n",
      "Epoch: 554, Loss: 0.01680004969239235\n",
      "Epoch: 555, Loss: 0.012199764139950275\n",
      "Epoch: 556, Loss: 0.013133133761584759\n",
      "Epoch: 557, Loss: 0.012550346553325653\n",
      "Epoch: 558, Loss: 0.01128689106553793\n",
      "Epoch: 559, Loss: 0.009554998017847538\n",
      "Epoch: 560, Loss: 0.009036632254719734\n",
      "Epoch: 561, Loss: 0.008320337161421776\n",
      "Epoch: 562, Loss: 0.008367765694856644\n",
      "Epoch: 563, Loss: 0.004715560935437679\n",
      "Epoch: 564, Loss: 0.005048171617090702\n",
      "Epoch: 565, Loss: 0.0037975655868649483\n",
      "Epoch: 566, Loss: 0.0065549155697226524\n",
      "Epoch: 567, Loss: 0.002172379055991769\n",
      "Epoch: 568, Loss: 0.006298054475337267\n",
      "Epoch: 569, Loss: 0.0010318157728761435\n",
      "Epoch: 570, Loss: 0.0008216665592044592\n",
      "Epoch: 571, Loss: 0.0013500479981303215\n",
      "Epoch: 572, Loss: 0.0016306181205436587\n",
      "Epoch: 573, Loss: 0.0010323203168809414\n",
      "Epoch: 574, Loss: 0.0005999738350510597\n",
      "Epoch: 575, Loss: 0.000342962535796687\n",
      "Epoch: 576, Loss: 0.00027151068206876516\n",
      "Epoch: 577, Loss: 0.0003323014243505895\n",
      "Epoch: 578, Loss: 0.000618841324467212\n",
      "Epoch: 579, Loss: 0.0002615909615997225\n",
      "Epoch: 580, Loss: 0.00023379124468192458\n",
      "Epoch: 581, Loss: 0.00031198846409097314\n",
      "Epoch: 582, Loss: 0.0002872782351914793\n",
      "Epoch: 583, Loss: 0.0002998329291585833\n",
      "Epoch: 584, Loss: 0.00019180004892405123\n",
      "Epoch: 585, Loss: 0.0002171699161408469\n",
      "Epoch: 586, Loss: 0.00014962771092541516\n",
      "Epoch: 587, Loss: 0.00014936912339180708\n",
      "Epoch: 588, Loss: 0.00015253540186677128\n",
      "Epoch: 589, Loss: 0.00018887802434619516\n",
      "Epoch: 590, Loss: 0.00015420648560393602\n",
      "Epoch: 591, Loss: 0.00013485750241670758\n",
      "Epoch: 592, Loss: 0.00012028674245811999\n",
      "Epoch: 593, Loss: 0.0001189853428513743\n",
      "Epoch: 594, Loss: 0.00013886985834687948\n",
      "Epoch: 595, Loss: 0.0001163290289696306\n",
      "Epoch: 596, Loss: 0.00010951609874609858\n",
      "Epoch: 597, Loss: 0.00010608695447444916\n",
      "Epoch: 598, Loss: 0.0001067499106284231\n",
      "Epoch: 599, Loss: 0.00010021639900514856\n",
      "Epoch: 600, Loss: 9.642405348131433e-05\n",
      "Epoch: 601, Loss: 9.156557644018903e-05\n",
      "Epoch: 602, Loss: 8.622041059425101e-05\n",
      "Epoch: 603, Loss: 9.312984184361994e-05\n",
      "Epoch: 604, Loss: 8.147240441758186e-05\n",
      "Epoch: 605, Loss: 8.554129453841597e-05\n",
      "Epoch: 606, Loss: 8.04294686531648e-05\n",
      "Epoch: 607, Loss: 8.780091593507677e-05\n",
      "Epoch: 608, Loss: 7.363232725765556e-05\n",
      "Epoch: 609, Loss: 7.484129309887066e-05\n",
      "Epoch: 610, Loss: 7.770108641125262e-05\n",
      "Epoch: 611, Loss: 7.258493860717863e-05\n",
      "Epoch: 612, Loss: 7.251348142744973e-05\n",
      "Epoch: 613, Loss: 7.09506421117112e-05\n",
      "Epoch: 614, Loss: 7.039038609946147e-05\n",
      "Epoch: 615, Loss: 6.864653551019728e-05\n",
      "Epoch: 616, Loss: 7.277425174834207e-05\n",
      "Epoch: 617, Loss: 6.357254460453987e-05\n",
      "Epoch: 618, Loss: 6.659351492999122e-05\n",
      "Epoch: 619, Loss: 6.40732905594632e-05\n",
      "Epoch: 620, Loss: 5.836628042743541e-05\n",
      "Epoch: 621, Loss: 8.322896610479802e-05\n",
      "Epoch: 622, Loss: 5.8225225075148046e-05\n",
      "Epoch: 623, Loss: 6.052761818864383e-05\n",
      "Epoch: 624, Loss: 5.5265594710363075e-05\n",
      "Epoch: 625, Loss: 5.995683386572637e-05\n",
      "Epoch: 626, Loss: 6.464093894464895e-05\n",
      "Epoch: 627, Loss: 5.466975562740117e-05\n",
      "Epoch: 628, Loss: 5.432110992842354e-05\n",
      "Epoch: 629, Loss: 5.567199332290329e-05\n",
      "Epoch: 630, Loss: 5.024399433750659e-05\n",
      "Epoch: 631, Loss: 4.9999751354334876e-05\n",
      "Epoch: 632, Loss: 6.297086656559259e-05\n",
      "Epoch: 633, Loss: 5.275385774439201e-05\n",
      "Epoch: 634, Loss: 5.1914292271248996e-05\n",
      "Epoch: 635, Loss: 5.386794873629697e-05\n",
      "Epoch: 636, Loss: 4.82586765429005e-05\n",
      "Epoch: 637, Loss: 4.8923728172667325e-05\n",
      "Epoch: 638, Loss: 5.0023805670207366e-05\n",
      "Epoch: 639, Loss: 4.955656550009735e-05\n",
      "Epoch: 640, Loss: 4.7618173994123936e-05\n",
      "Epoch: 641, Loss: 4.650306436815299e-05\n",
      "Epoch: 642, Loss: 4.557203283184208e-05\n",
      "Epoch: 643, Loss: 4.550185985863209e-05\n",
      "Epoch: 644, Loss: 4.466444079298526e-05\n",
      "Epoch: 645, Loss: 4.469136911211535e-05\n",
      "Epoch: 646, Loss: 4.384899148135446e-05\n",
      "Epoch: 647, Loss: 4.507653648033738e-05\n",
      "Epoch: 648, Loss: 4.6577973989769816e-05\n",
      "Epoch: 649, Loss: 4.439863550942391e-05\n",
      "Epoch: 650, Loss: 4.1206243622582406e-05\n",
      "Epoch: 651, Loss: 4.328784052631818e-05\n",
      "Epoch: 652, Loss: 4.184215868008323e-05\n",
      "Epoch: 653, Loss: 3.980130350100808e-05\n",
      "Epoch: 654, Loss: 4.370506212580949e-05\n",
      "Epoch: 655, Loss: 4.216459274175577e-05\n",
      "Epoch: 656, Loss: 4.224709118716419e-05\n",
      "Epoch: 657, Loss: 4.030699346913025e-05\n",
      "Epoch: 658, Loss: 4.168596933595836e-05\n",
      "Epoch: 659, Loss: 4.048593109473586e-05\n",
      "Epoch: 660, Loss: 3.905562698491849e-05\n",
      "Epoch: 661, Loss: 3.87205473089125e-05\n",
      "Epoch: 662, Loss: 3.8586720620514825e-05\n",
      "Epoch: 663, Loss: 3.74739793187473e-05\n",
      "Epoch: 664, Loss: 4.017532410216518e-05\n",
      "Epoch: 665, Loss: 3.8739144656574354e-05\n",
      "Epoch: 666, Loss: 3.8434809539467096e-05\n",
      "Epoch: 667, Loss: 3.794852818828076e-05\n",
      "Epoch: 668, Loss: 3.633219967014156e-05\n",
      "Epoch: 669, Loss: 4.06890940212179e-05\n",
      "Epoch: 670, Loss: 3.80495548597537e-05\n",
      "Epoch: 671, Loss: 3.454553007031791e-05\n",
      "Epoch: 672, Loss: 3.413555896258913e-05\n",
      "Epoch: 673, Loss: 3.78540025849361e-05\n",
      "Epoch: 674, Loss: 3.392264625290409e-05\n",
      "Epoch: 675, Loss: 3.3929940400412306e-05\n",
      "Epoch: 676, Loss: 3.638599446276203e-05\n",
      "Epoch: 677, Loss: 3.3868109312606975e-05\n",
      "Epoch: 678, Loss: 3.298137380625121e-05\n",
      "Epoch: 679, Loss: 3.481438761809841e-05\n",
      "Epoch: 680, Loss: 3.2907268177950755e-05\n",
      "Epoch: 681, Loss: 3.279678276157938e-05\n",
      "Epoch: 682, Loss: 3.2041643862612545e-05\n",
      "Epoch: 683, Loss: 3.1110572308534756e-05\n",
      "Epoch: 684, Loss: 3.223353996872902e-05\n",
      "Epoch: 685, Loss: 3.057624417124316e-05\n",
      "Epoch: 686, Loss: 3.241013837396167e-05\n",
      "Epoch: 687, Loss: 3.142745481454767e-05\n",
      "Epoch: 688, Loss: 3.0233984944061376e-05\n",
      "Epoch: 689, Loss: 3.217603079974651e-05\n",
      "Epoch: 690, Loss: 3.0609051464125514e-05\n",
      "Epoch: 691, Loss: 3.297155490145087e-05\n",
      "Epoch: 692, Loss: 2.961463906103745e-05\n",
      "Epoch: 693, Loss: 3.23387430398725e-05\n",
      "Epoch: 694, Loss: 2.9472916139638983e-05\n",
      "Epoch: 695, Loss: 2.9595710657304153e-05\n",
      "Epoch: 696, Loss: 2.8032352929585613e-05\n",
      "Epoch: 697, Loss: 2.961172504001297e-05\n",
      "Epoch: 698, Loss: 2.8248208764125593e-05\n",
      "Epoch: 699, Loss: 2.8566422770381905e-05\n",
      "Epoch: 700, Loss: 2.8431335522327572e-05\n",
      "Epoch: 701, Loss: 2.894516546803061e-05\n",
      "Epoch: 702, Loss: 2.7608612072071992e-05\n",
      "Epoch: 703, Loss: 2.8453898266889155e-05\n",
      "Epoch: 704, Loss: 3.0016517484909855e-05\n",
      "Epoch: 705, Loss: 2.708163628994953e-05\n",
      "Epoch: 706, Loss: 2.8401587769621983e-05\n",
      "Epoch: 707, Loss: 2.6069126761285588e-05\n",
      "Epoch: 708, Loss: 2.754025081230793e-05\n",
      "Epoch: 709, Loss: 2.6633852030499838e-05\n",
      "Epoch: 710, Loss: 2.5054456273210235e-05\n",
      "Epoch: 711, Loss: 2.6817035177373327e-05\n",
      "Epoch: 712, Loss: 2.571076765889302e-05\n",
      "Epoch: 713, Loss: 2.6284964405931532e-05\n",
      "Epoch: 714, Loss: 2.6444040486239828e-05\n",
      "Epoch: 715, Loss: 3.3712396543705836e-05\n",
      "Epoch: 716, Loss: 2.5428740627830848e-05\n",
      "Epoch: 717, Loss: 2.4706227122806013e-05\n",
      "Epoch: 718, Loss: 2.475345172570087e-05\n",
      "Epoch: 719, Loss: 2.4206936359405518e-05\n",
      "Epoch: 720, Loss: 2.6593688744469546e-05\n",
      "Epoch: 721, Loss: 2.365591899433639e-05\n",
      "Epoch: 722, Loss: 2.519747977203224e-05\n",
      "Epoch: 723, Loss: 2.3695905838394538e-05\n",
      "Epoch: 724, Loss: 2.3232207240653224e-05\n",
      "Epoch: 725, Loss: 2.405644227110315e-05\n",
      "Epoch: 726, Loss: 2.30264849960804e-05\n",
      "Epoch: 727, Loss: 2.4933713575592265e-05\n",
      "Epoch: 728, Loss: 2.290950396854896e-05\n",
      "Epoch: 729, Loss: 2.192169449699577e-05\n",
      "Epoch: 730, Loss: 2.3628314011148177e-05\n",
      "Epoch: 731, Loss: 2.190934173995629e-05\n",
      "Epoch: 732, Loss: 2.2567730411537923e-05\n",
      "Epoch: 733, Loss: 2.301706263097003e-05\n",
      "Epoch: 734, Loss: 2.242465052404441e-05\n",
      "Epoch: 735, Loss: 2.1705103790736757e-05\n",
      "Epoch: 736, Loss: 2.1795975044369698e-05\n",
      "Epoch: 737, Loss: 2.1480478608282283e-05\n",
      "Epoch: 738, Loss: 2.126317485817708e-05\n",
      "Epoch: 739, Loss: 2.1713782189181075e-05\n",
      "Epoch: 740, Loss: 2.2673186322208494e-05\n",
      "Epoch: 741, Loss: 2.0170600691926666e-05\n",
      "Epoch: 742, Loss: 2.0071824110345915e-05\n",
      "Epoch: 743, Loss: 2.0055804270668887e-05\n",
      "Epoch: 744, Loss: 1.9747676560655236e-05\n",
      "Epoch: 745, Loss: 2.0248404325684533e-05\n",
      "Epoch: 746, Loss: 2.0663434042944573e-05\n",
      "Epoch: 747, Loss: 1.9523802620824426e-05\n",
      "Epoch: 748, Loss: 2.00187732843915e-05\n",
      "Epoch: 749, Loss: 1.908406375150662e-05\n",
      "Epoch: 750, Loss: 1.9070979760726914e-05\n",
      "Epoch: 751, Loss: 1.9714942027349025e-05\n",
      "Epoch: 752, Loss: 1.835356124502141e-05\n",
      "Epoch: 753, Loss: 1.9597908249124885e-05\n",
      "Epoch: 754, Loss: 1.902589065139182e-05\n",
      "Epoch: 755, Loss: 1.8671193174668588e-05\n",
      "Epoch: 756, Loss: 1.8747494323179126e-05\n",
      "Epoch: 757, Loss: 1.9140736185363494e-05\n",
      "Epoch: 758, Loss: 1.8553439076640643e-05\n",
      "Epoch: 759, Loss: 1.788979716366157e-05\n",
      "Epoch: 760, Loss: 1.795743446564302e-05\n",
      "Epoch: 761, Loss: 1.8461862055119127e-05\n",
      "Epoch: 762, Loss: 1.70815455931006e-05\n",
      "Epoch: 763, Loss: 1.7481326722190715e-05\n",
      "Epoch: 764, Loss: 1.7406460756319575e-05\n",
      "Epoch: 765, Loss: 1.8478425772627816e-05\n",
      "Epoch: 766, Loss: 1.7418766219634563e-05\n",
      "Epoch: 767, Loss: 1.7074980860343203e-05\n",
      "Epoch: 768, Loss: 1.6305270037264563e-05\n",
      "Epoch: 769, Loss: 1.6092282749013975e-05\n",
      "Epoch: 770, Loss: 1.612643973203376e-05\n",
      "Epoch: 771, Loss: 1.6142434105859138e-05\n",
      "Epoch: 772, Loss: 1.598979542904999e-05\n",
      "Epoch: 773, Loss: 1.6078472981462255e-05\n",
      "Epoch: 774, Loss: 1.6725372915971093e-05\n",
      "Epoch: 775, Loss: 1.6745700122555718e-05\n",
      "Epoch: 776, Loss: 1.5581303159706295e-05\n",
      "Epoch: 777, Loss: 1.6023219359340146e-05\n",
      "Epoch: 778, Loss: 1.5756411812617444e-05\n",
      "Epoch: 779, Loss: 1.531744965177495e-05\n",
      "Epoch: 780, Loss: 1.5395216905744746e-05\n",
      "Epoch: 781, Loss: 1.5209133380267303e-05\n",
      "Epoch: 782, Loss: 1.5506422641919926e-05\n",
      "Epoch: 783, Loss: 1.494454136263812e-05\n",
      "Epoch: 784, Loss: 1.5025981156213675e-05\n",
      "Epoch: 785, Loss: 1.536032505100593e-05\n",
      "Epoch: 786, Loss: 1.592000262462534e-05\n",
      "Epoch: 787, Loss: 1.4690892385260668e-05\n",
      "Epoch: 788, Loss: 1.4570223356713541e-05\n",
      "Epoch: 789, Loss: 1.4187165106704924e-05\n",
      "Epoch: 790, Loss: 1.587844599271193e-05\n",
      "Epoch: 791, Loss: 1.3717620277020615e-05\n",
      "Epoch: 792, Loss: 1.3966186088509858e-05\n",
      "Epoch: 793, Loss: 1.4078124877414666e-05\n",
      "Epoch: 794, Loss: 1.428820178261958e-05\n",
      "Epoch: 795, Loss: 1.417409202986164e-05\n",
      "Epoch: 796, Loss: 1.4192222806741484e-05\n",
      "Epoch: 797, Loss: 1.3309120731719304e-05\n",
      "Epoch: 798, Loss: 1.3531529475585558e-05\n",
      "Epoch: 799, Loss: 1.3452312487061135e-05\n",
      "Epoch: 800, Loss: 1.3131759260431863e-05\n",
      "Epoch: 801, Loss: 1.3052535905444529e-05\n",
      "Epoch: 802, Loss: 1.3304759704624303e-05\n",
      "Epoch: 803, Loss: 1.3126667909091339e-05\n",
      "Epoch: 804, Loss: 1.3240792213764507e-05\n",
      "Epoch: 805, Loss: 1.2192649592179805e-05\n",
      "Epoch: 806, Loss: 1.3006732842768542e-05\n",
      "Epoch: 807, Loss: 1.2880955182481557e-05\n",
      "Epoch: 808, Loss: 1.2091616554243956e-05\n",
      "Epoch: 809, Loss: 1.2206462997710332e-05\n",
      "Epoch: 810, Loss: 1.2814104593417142e-05\n",
      "Epoch: 811, Loss: 1.2562621122924611e-05\n",
      "Epoch: 812, Loss: 1.2429587513906881e-05\n",
      "Epoch: 813, Loss: 1.1797963452409022e-05\n",
      "Epoch: 814, Loss: 1.2087974937458057e-05\n",
      "Epoch: 815, Loss: 1.1754344086511992e-05\n",
      "Epoch: 816, Loss: 1.1864106454595458e-05\n",
      "Epoch: 817, Loss: 1.1680925126711372e-05\n",
      "Epoch: 818, Loss: 1.195930872199824e-05\n",
      "Epoch: 819, Loss: 1.1869908121298067e-05\n",
      "Epoch: 820, Loss: 1.2028350283799227e-05\n",
      "Epoch: 821, Loss: 1.1475221981527284e-05\n",
      "Epoch: 822, Loss: 1.1330575944157317e-05\n",
      "Epoch: 823, Loss: 1.1629282198555302e-05\n",
      "Epoch: 824, Loss: 1.0866835509659722e-05\n",
      "Epoch: 825, Loss: 1.187281304737553e-05\n",
      "Epoch: 826, Loss: 1.1100884876213968e-05\n",
      "Epoch: 827, Loss: 1.1163400813529734e-05\n",
      "Epoch: 828, Loss: 1.0624775313772261e-05\n",
      "Epoch: 829, Loss: 1.0765073056973051e-05\n",
      "Epoch: 830, Loss: 1.13966916615027e-05\n",
      "Epoch: 831, Loss: 1.1154656021972187e-05\n",
      "Epoch: 832, Loss: 1.1245537280046847e-05\n",
      "Epoch: 833, Loss: 1.04496093626949e-05\n",
      "Epoch: 834, Loss: 1.0336940249544568e-05\n",
      "Epoch: 835, Loss: 1.0361655768065248e-05\n",
      "Epoch: 836, Loss: 1.0019300134445075e-05\n",
      "Epoch: 837, Loss: 9.663867786002811e-06\n",
      "Epoch: 838, Loss: 1.0161045793211088e-05\n",
      "Epoch: 839, Loss: 9.581730409990996e-06\n",
      "Epoch: 840, Loss: 9.99314033833798e-06\n",
      "Epoch: 841, Loss: 1.0065822607430164e-05\n",
      "Epoch: 842, Loss: 9.745997886057012e-06\n",
      "Epoch: 843, Loss: 1.002874159894418e-05\n",
      "Epoch: 844, Loss: 9.03294039744651e-06\n",
      "Epoch: 845, Loss: 9.522850632492919e-06\n",
      "Epoch: 846, Loss: 9.325866813014727e-06\n",
      "Epoch: 847, Loss: 9.173945727525279e-06\n",
      "Epoch: 848, Loss: 9.314965609519277e-06\n",
      "Epoch: 849, Loss: 9.087448233913165e-06\n",
      "Epoch: 850, Loss: 9.067099199455697e-06\n",
      "Epoch: 851, Loss: 9.165228220808785e-06\n",
      "Epoch: 852, Loss: 9.067099199455697e-06\n",
      "Epoch: 853, Loss: 9.068548934010323e-06\n",
      "Epoch: 854, Loss: 9.009677341964561e-06\n",
      "Epoch: 855, Loss: 9.1056263045175e-06\n",
      "Epoch: 856, Loss: 8.745823834033217e-06\n",
      "Epoch: 857, Loss: 8.4536159192794e-06\n",
      "Epoch: 858, Loss: 8.59826832311228e-06\n",
      "Epoch: 859, Loss: 9.17104262043722e-06\n",
      "Epoch: 860, Loss: 8.262451046903152e-06\n",
      "Epoch: 861, Loss: 8.766176506469492e-06\n",
      "Epoch: 862, Loss: 7.989874575287104e-06\n",
      "Epoch: 863, Loss: 8.04802493803436e-06\n",
      "Epoch: 864, Loss: 8.292981874546967e-06\n",
      "Epoch: 865, Loss: 8.324236659973394e-06\n",
      "Epoch: 866, Loss: 8.193399480660446e-06\n",
      "Epoch: 867, Loss: 8.290053301607259e-06\n",
      "Epoch: 868, Loss: 8.069103387242649e-06\n",
      "Epoch: 869, Loss: 8.224653356592171e-06\n",
      "Epoch: 870, Loss: 7.821236067684367e-06\n",
      "Epoch: 871, Loss: 7.795067176630255e-06\n",
      "Epoch: 872, Loss: 7.763816938677337e-06\n",
      "Epoch: 873, Loss: 7.4527138167468365e-06\n",
      "Epoch: 874, Loss: 7.845952495699748e-06\n",
      "Epoch: 875, Loss: 7.473787718481617e-06\n",
      "Epoch: 876, Loss: 7.5014108915638644e-06\n",
      "Epoch: 877, Loss: 7.674402695556637e-06\n",
      "Epoch: 878, Loss: 7.6555061241379e-06\n",
      "Epoch: 879, Loss: 7.528306923632044e-06\n",
      "Epoch: 880, Loss: 7.422182989103021e-06\n",
      "Epoch: 881, Loss: 7.2375560193904676e-06\n",
      "Epoch: 882, Loss: 7.3589462772361e-06\n",
      "Epoch: 883, Loss: 7.119076599337859e-06\n",
      "Epoch: 884, Loss: 7.334958354476839e-06\n",
      "Epoch: 885, Loss: 6.880661658215104e-06\n",
      "Epoch: 886, Loss: 7.6067944974056445e-06\n",
      "Epoch: 887, Loss: 7.090720373525983e-06\n",
      "Epoch: 888, Loss: 6.9286297730286606e-06\n",
      "Epoch: 889, Loss: 6.708391538268188e-06\n",
      "Epoch: 890, Loss: 6.831957307440462e-06\n",
      "Epoch: 891, Loss: 6.5135868680954445e-06\n",
      "Epoch: 892, Loss: 6.596450475626625e-06\n",
      "Epoch: 893, Loss: 6.762905286450405e-06\n",
      "Epoch: 894, Loss: 6.681492777715903e-06\n",
      "Epoch: 895, Loss: 6.68730353936553e-06\n",
      "Epoch: 896, Loss: 6.284620667429408e-06\n",
      "Epoch: 897, Loss: 6.383473646565108e-06\n",
      "Epoch: 898, Loss: 6.342769665934611e-06\n",
      "Epoch: 899, Loss: 9.015645446197595e-06\n",
      "Epoch: 900, Loss: 6.364573437167564e-06\n",
      "Epoch: 901, Loss: 6.432904683606466e-06\n",
      "Epoch: 902, Loss: 5.937900368735427e-06\n",
      "Epoch: 903, Loss: 6.10289998803637e-06\n",
      "Epoch: 904, Loss: 6.2431881815427914e-06\n",
      "Epoch: 905, Loss: 6.200301868375391e-06\n",
      "Epoch: 906, Loss: 6.082545951358043e-06\n",
      "Epoch: 907, Loss: 6.175588623591466e-06\n",
      "Epoch: 908, Loss: 5.894287824048661e-06\n",
      "Epoch: 909, Loss: 5.703844180970918e-06\n",
      "Epoch: 910, Loss: 5.802699433843372e-06\n",
      "Epoch: 911, Loss: 5.7990664572571404e-06\n",
      "Epoch: 912, Loss: 5.643516942654969e-06\n",
      "Epoch: 913, Loss: 5.6086255426635034e-06\n",
      "Epoch: 914, Loss: 5.762722594226943e-06\n",
      "Epoch: 915, Loss: 5.436354967969237e-06\n",
      "Epoch: 916, Loss: 5.469063580676448e-06\n",
      "Epoch: 917, Loss: 5.445804163173307e-06\n",
      "Epoch: 918, Loss: 5.921893716731574e-06\n",
      "Epoch: 919, Loss: 5.424724349722965e-06\n",
      "Epoch: 920, Loss: 5.34185846845503e-06\n",
      "Epoch: 921, Loss: 5.413092821981991e-06\n",
      "Epoch: 922, Loss: 5.402189344749786e-06\n",
      "Epoch: 923, Loss: 5.251727088761982e-06\n",
      "Epoch: 924, Loss: 5.27862221133546e-06\n",
      "Epoch: 925, Loss: 5.264082574285567e-06\n",
      "Epoch: 926, Loss: 5.1535971579141915e-06\n",
      "Epoch: 927, Loss: 5.333863555279095e-06\n",
      "Epoch: 928, Loss: 5.205934030527715e-06\n",
      "Epoch: 929, Loss: 5.06419246448786e-06\n",
      "Epoch: 930, Loss: 5.248814431979554e-06\n",
      "Epoch: 931, Loss: 5.003861133445753e-06\n",
      "Epoch: 932, Loss: 4.950071797793498e-06\n",
      "Epoch: 933, Loss: 5.131785201228922e-06\n",
      "Epoch: 934, Loss: 4.908639311906882e-06\n",
      "Epoch: 935, Loss: 4.920997525914572e-06\n",
      "Epoch: 936, Loss: 4.871568307862617e-06\n",
      "Epoch: 937, Loss: 4.779981736646732e-06\n",
      "Epoch: 938, Loss: 4.957339115208015e-06\n",
      "Epoch: 939, Loss: 4.776346941071097e-06\n",
      "Epoch: 940, Loss: 4.82940731671988e-06\n",
      "Epoch: 941, Loss: 4.7051134970388375e-06\n",
      "Epoch: 942, Loss: 4.663681011152221e-06\n",
      "Epoch: 943, Loss: 4.622249889507657e-06\n",
      "Epoch: 944, Loss: 4.622249434760306e-06\n",
      "Epoch: 945, Loss: 4.422357051225845e-06\n",
      "Epoch: 946, Loss: 4.732732577394927e-06\n",
      "Epoch: 947, Loss: 4.558281943900511e-06\n",
      "Epoch: 948, Loss: 4.484868441068102e-06\n",
      "Epoch: 949, Loss: 4.268258635420352e-06\n",
      "Epoch: 950, Loss: 4.281341261958005e-06\n",
      "Epoch: 951, Loss: 4.427444309840212e-06\n",
      "Epoch: 952, Loss: 4.249359335517511e-06\n",
      "Epoch: 953, Loss: 4.385285592434229e-06\n",
      "Epoch: 954, Loss: 4.156317572778789e-06\n",
      "Epoch: 955, Loss: 4.267531039658934e-06\n",
      "Epoch: 956, Loss: 4.144687864027219e-06\n",
      "Epoch: 957, Loss: 4.106890173716238e-06\n",
      "Epoch: 958, Loss: 3.96660379919922e-06\n",
      "Epoch: 959, Loss: 4.167221504758345e-06\n",
      "Epoch: 960, Loss: 4.271164470992517e-06\n",
      "Epoch: 961, Loss: 3.945523076254176e-06\n",
      "Epoch: 962, Loss: 4.067638656124473e-06\n",
      "Epoch: 963, Loss: 3.993497557530645e-06\n",
      "Epoch: 964, Loss: 3.988408934674226e-06\n",
      "Epoch: 965, Loss: 3.965874839195749e-06\n",
      "Epoch: 966, Loss: 3.995677161583444e-06\n",
      "Epoch: 967, Loss: 3.829949037026381e-06\n",
      "Epoch: 968, Loss: 3.7797935874550603e-06\n",
      "Epoch: 969, Loss: 3.670034629976726e-06\n",
      "Epoch: 970, Loss: 4.18829267800902e-06\n",
      "Epoch: 971, Loss: 3.7972395148244686e-06\n",
      "Epoch: 972, Loss: 3.7092863749421667e-06\n",
      "Epoch: 973, Loss: 3.8103228234831477e-06\n",
      "Epoch: 974, Loss: 3.717282424986479e-06\n",
      "Epoch: 975, Loss: 3.581356622817111e-06\n",
      "Epoch: 976, Loss: 3.574086349544814e-06\n",
      "Epoch: 977, Loss: 3.586444336178829e-06\n",
      "Epoch: 978, Loss: 3.4308916383452015e-06\n",
      "Epoch: 979, Loss: 3.6424135032575577e-06\n",
      "Epoch: 980, Loss: 3.4752313240460353e-06\n",
      "Epoch: 981, Loss: 3.4425211197230965e-06\n",
      "Epoch: 982, Loss: 3.5479181406117277e-06\n",
      "Epoch: 983, Loss: 3.3284006804024102e-06\n",
      "Epoch: 984, Loss: 3.4454278647899628e-06\n",
      "Epoch: 985, Loss: 3.3502053611300653e-06\n",
      "Epoch: 986, Loss: 3.34729907081055e-06\n",
      "Epoch: 987, Loss: 3.363291170899174e-06\n",
      "Epoch: 988, Loss: 3.3836431612144224e-06\n",
      "Epoch: 989, Loss: 3.384369392733788e-06\n",
      "Epoch: 990, Loss: 3.89532897315803e-06\n",
      "Epoch: 991, Loss: 3.669291345431702e-06\n",
      "Epoch: 992, Loss: 3.1176050470094196e-06\n",
      "Epoch: 993, Loss: 3.1372310331789777e-06\n",
      "Epoch: 994, Loss: 3.1277816106012324e-06\n",
      "Epoch: 995, Loss: 3.1110632789932424e-06\n",
      "Epoch: 996, Loss: 3.0972526019468205e-06\n",
      "Epoch: 997, Loss: 3.1713939279143233e-06\n",
      "Epoch: 998, Loss: 2.959871608254616e-06\n",
      "Epoch: 999, Loss: 2.9656864626304014e-06\n",
      "Epoch: 1000, Loss: 3.0710843930137344e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transformer = Transformer( tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(source_data)\n",
    "    \n",
    "    # Adjust target_data to have the same length as model output\n",
    "    target_data_adjusted = target_data[:, :output.size(1)]\n",
    "    \n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), target_data_adjusted.contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence:  ⁇  day, a little girl named lily found a needle in her room. she knew it was difficult to play with it because it was sharp. lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "def word_to_token_id(word, sp_model):\n",
    "    # Convert word to token ID using SentencePiece\n",
    "    return sp_model.piece_to_id(word)\n",
    "\n",
    "def generate_text(model, sp_model, starting_word, ending_word, max_length=50):\n",
    "    # Convert starting and ending words to token IDs\n",
    "    starting_token_id = word_to_token_id(starting_word, sp_model)\n",
    "    if starting_token_id is None:\n",
    "        raise ValueError(f\"Starting word '{starting_word}' not found in vocabulary.\")\n",
    "    ending_token_id = word_to_token_id(ending_word, sp_model)\n",
    "    if ending_token_id is None:\n",
    "        raise ValueError(f\"Ending word '{ending_word}' not found in vocabulary.\")\n",
    "    \n",
    "    generated_sequence = [starting_token_id]\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            input_tensor = torch.tensor([generated_sequence])\n",
    "            output = model(input_tensor)\n",
    "            predicted_token = output.argmax(-1)[:,-1].item()\n",
    "            generated_sequence.append(predicted_token)\n",
    "            if predicted_token == ending_token_id:\n",
    "                break\n",
    "    \n",
    "    # Convert token IDs to words using SentencePiece\n",
    "    generated_text = sp_model.decode_ids(generated_sequence)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "starting_word = \"needle in\"\n",
    "ending_word = \"</sos>\"\n",
    "generated_sequence = generate_text(transformer, sp, starting_word, ending_word)\n",
    "print(\"Generated sequence:\", generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
    "\n",
    "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
    "\n",
    "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing \n",
    "the needle and fixing her shirt. They both felt happy because they had shared and worked together"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
