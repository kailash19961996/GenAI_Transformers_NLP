{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How tokens are prepared - rough idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input sequences are [['The', 'dog', 'ran', 'into'], ['The', 'cat', 'sat', 'in', 'the']]\n",
      "The labels are [['dog', 'ran', 'into', 'street', '<EOS>'], ['cat', 'sat', 'in', 'the', 'home', '<EOS>']]\n"
     ]
    }
   ],
   "source": [
    "stories = [\"The dog ran into street\", \"The cat sat in the home\"]\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "def create_input_labels(stories, eos_token = '<EOS>'):\n",
    "    inputs =[]\n",
    "    labels = []\n",
    "\n",
    "    for story in stories:\n",
    "        tokens = tokenize(story)\n",
    "        inputs.append(tokens[:-1]) # leaving the last word\n",
    "        labels.append(tokens[1:] + [eos_token])\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "input_sequences, label_sequences = create_input_labels(stories)\n",
    "\n",
    "print(f\"The input sequences are {input_sequences}\")\n",
    "print(f\"The labels are {label_sequences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positonal Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Positional Encoding?\n",
    "\n",
    "Problem: Transformers, unlike RNNs, don't have an inherent understanding of word order within a sequence. They process words without considering their position relative to each other.\n",
    "\n",
    "Solution: Positional encodings are vectors added to the input word embeddings to inject information about the position of each word in the sequence. This helps the model distinguish between words in different places, even if they are the same word.\n",
    "\n",
    "### Why is it Important?\n",
    "\n",
    "Language Structure: Word order is crucial for understanding the meaning of sentences. (\"The dog chased the cat\" is different from \"The cat chased the dog\").\n",
    "\n",
    "Self-Attention: Within Transformers, the attention mechanism needs positional information to understand how different words relate to each other based on their distance and order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([8.41470985e-01, 5.40302306e-01, 2.51162229e-02, 9.99684538e-01,\n",
       "       6.30957303e-04, 9.99999801e-01, 1.58489319e-05, 1.00000000e+00,\n",
       "       3.98107171e-07, 1.00000000e+00])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def positional_encoding(max_position, embedding_dim):\n",
    "    position_enc = np.zeros((max_position, embedding_dim))\n",
    "    \n",
    "    for pos in range(max_position):\n",
    "        for i in range(0, embedding_dim, 2):\n",
    "            position_enc[pos, i] = np.sin(pos / (10000 ** (2 * i / embedding_dim)))\n",
    "            position_enc[pos, i + 1] = np.cos(pos / (10000 ** (2 * i / embedding_dim)))\n",
    "\n",
    "    return position_enc\n",
    "\n",
    "# Example usage\n",
    "max_seq_length = 100 \n",
    "embedding_dim = 10  # Example dimensions\n",
    "\n",
    "pos_encoding = positional_encoding(max_seq_length, embedding_dim)\n",
    "print(pos_encoding.shape)  # Output: (100, 512)\n",
    "pos_encoding[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## where positional encoding is used in transformers architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class TransformerDecoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, embedding_dim, num_layers, dff, num_heads, dropout_rate):\n",
    "    super(TransformerDecoder, self).__init__()\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.num_layers = num_layers\n",
    "    self.dff = dff\n",
    "    self.num_heads = num_heads\n",
    "    self.dropout_rate = dropout_rate\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.pos_encoding = self._positional_encoding(max_len=5000, d_model=embedding_dim)  # Example max_len\n",
    "\n",
    "    self.decoder_layers = [self._get_decoder_layer() for _ in range(num_layers)]\n",
    "    self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  # Define positional encoding function (same as previous example)\n",
    "  def _positional_encoding(self, max_len, d_model):\n",
    "    position_enc = np.zeros((max_len, d_model))\n",
    "    for pos in range(max_len):\n",
    "      for i in range(0, d_model, 2):\n",
    "        position_enc[pos, i] = np.sin(pos / (10000 ** (2 * i / d_model)))\n",
    "        position_enc[pos, i + 1] = np.cos(pos / (10000 ** (2 * i / d_model)))\n",
    "    return position_enc\n",
    "\n",
    "  def _get_decoder_layer(self):\n",
    "    # Masked self-attention layer with masking for decoder\n",
    "    self_attention = tf.keras.layers.MultiHeadAttention(num_heads=self.num_heads, \n",
    "                                                     dropout_rate=self.dropout_rate)\n",
    "    # Encoder-decoder attention (optional, not shown here for simplicity)\n",
    "\n",
    "    # Feed forward network\n",
    "    feed_forward = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(self.dff, activation=\"relu\"),  # First layer\n",
    "        tf.keras.layers.Dense(self.embedding_dim),  # Second layer\n",
    "        tf.keras.layers.Dropout(self.dropout_rate)\n",
    "    ])\n",
    "\n",
    "    # Layer normalization\n",
    "    layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    # Residual connections and dropout\n",
    "    dropout1 = tf.keras.layers.Dropout(self.dropout_rate)\n",
    "    dropout2 = tf.keras.layers.Dropout(self.dropout_rate)\n",
    "\n",
    "    return tf.keras.Sequential([\n",
    "        self_attention,\n",
    "        layer_norm1,\n",
    "        dropout1,\n",
    "        feed_forward,\n",
    "        layer_norm2,\n",
    "        dropout2\n",
    "    ])\n",
    "\n",
    "  def call(self, inputs, training=False):\n",
    "    # Lookup word embeddings\n",
    "    embedded = self.embedding(inputs)\n",
    "\n",
    "    # Add positional encoding to embeddings (assuming batch_size is the first dimension)\n",
    "    seq_len = tf.shape(inputs)[1]\n",
    "    position_enc = tf.cast(self.pos_encoding[:seq_len], tf.float32)  # Slice relevant positions\n",
    "    embedded = embedded + position_enc\n",
    "\n",
    "    # Pass through decoder layers\n",
    "    for layer in self.decoder_layers:\n",
    "      embedded = layer(embedded, training=training)\n",
    "\n",
    "    # Final dense layer for output logits\n",
    "    output = self.final_layer(embedded)\n",
    "    return output\n",
    "\n",
    "# Example usage (assuming you have tokenized input sequences)\n",
    "model = TransformerDecoder(vocab_size=8000, embedding_dim=512, num_layers=2, dff=2048, num_heads=8, dropout_rate=0.1)\n",
    "decoder_output = model(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfodj",
   "language": "python",
   "name": "tfodj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
