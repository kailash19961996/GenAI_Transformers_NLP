{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = datasets.load_from_disk(\"tiny_stories_train\").select(range(10))\n",
    "val_dataset = datasets.load_from_disk(\"tiny_stories_val\").select(range(2))\n",
    "\n",
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inputs_and_labels(train_data, sp):\n",
    "    \n",
    "    # Convert input and label sequences to strings\n",
    "    input_strings = train_data\n",
    "    label_strings = train_data\n",
    "\n",
    "    # Tokenize input strings and add <sos> token at the beginning\n",
    "    tokenized_inputs = []\n",
    "    input_ids = []\n",
    "    for sequence in input_strings:\n",
    "        tokenized_sequence = sp.encode_as_pieces(sequence)\n",
    "        tokenized_sequence = ['<sos>'] + tokenized_sequence  # Add <sos> token manually\n",
    "        tokenized_inputs.append(tokenized_sequence)\n",
    "        input_ids.append([sp.piece_to_id('<sos>')] + sp.encode_as_ids(sequence))  # Get token IDs\n",
    "\n",
    "    # Tokenize label strings and add </sos> token at the end\n",
    "    tokenized_labels = []\n",
    "    label_ids = []\n",
    "    for sequence in label_strings:\n",
    "        tokenized_sequence = sp.encode_as_pieces(sequence)\n",
    "        tokenized_sequence.append('</sos>')  # Add </sos> token manually at the end\n",
    "        tokenized_labels.append(tokenized_sequence)\n",
    "        label_ids.append(sp.encode_as_ids(sequence) + [sp.piece_to_id('</sos>')])  # Get token IDs\n",
    "        \n",
    "    # Print tokenized input and label sequences\n",
    "    for i in range(len(inputs)):\n",
    "        print(\"Input Text:\", input_strings[i])\n",
    "        print(\"Tokenized Input:\", tokenized_inputs[i])\n",
    "        print(\"Input IDs:\", input_ids[i])\n",
    "        print(\"Label Text:\", label_strings[i])\n",
    "        print(\"Tokenized Label:\", tokenized_labels[i])\n",
    "        print(\"Label IDs:\", label_ids[i])\n",
    "\n",
    "    return input_ids, label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Not found: \"small_m.model\": No such file or directory Error #2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sp \u001b[38;5;241m=\u001b[39m spm\u001b[38;5;241m.\u001b[39mSentencePieceProcessor()\n\u001b[0;32m----> 3\u001b[0m \u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msmall_m.model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m source_ids, target_ids \u001b[38;5;241m=\u001b[39m generate_inputs_and_labels(train_df, sp)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sentencepiece/__init__.py:961\u001b[0m, in \u001b[0;36mSentencePieceProcessor.Load\u001b[0;34m(self, model_file, model_proto)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_proto:\n\u001b[1;32m    960\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLoadFromSerializedProto(model_proto)\n\u001b[0;32m--> 961\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadFromFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sentencepiece/__init__.py:316\u001b[0m, in \u001b[0;36mSentencePieceProcessor.LoadFromFile\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mLoadFromFile\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sentencepiece\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePieceProcessor_LoadFromFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOSError\u001b[0m: Not found: \"small_m.model\": No such file or directory Error #2"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('small_m.model')\n",
    "\n",
    "source_ids, target_ids = generate_inputs_and_labels(train_dataset, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.max_seq_length = max_seq_length\n",
    "        pe = self._init_pe(max_seq_length, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_length = x.size(1)\n",
    "        pe = self.pe[:seq_length, :] if seq_length <= self.max_seq_length else self._init_pe(seq_length, self.d_model)\n",
    "        return x + pe\n",
    "\n",
    "    def _init_pe(self, seq_length, d_model):\n",
    "        pe = torch.zeros(seq_length, d_model)\n",
    "        position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        return pe\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, decoder_input, decoder_mask):\n",
    "        attn_output = self.self_attn(decoder_input, decoder_input, decoder_input, decoder_mask)\n",
    "        x = self.norm1(decoder_input + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "   \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, tgt):\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return tgt_mask\n",
    "\n",
    "    def forward(self, decoder_input):\n",
    "        decoder_mask = self.generate_mask(decoder_input)\n",
    "        decoder_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(decoder_input)))\n",
    "\n",
    "        dec_output = decoder_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, decoder_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output\n",
    "\n",
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to tensors\n",
    "source_data = torch.tensor(source_ids)\n",
    "target_data = torch.tensor(target_ids)\n",
    "\n",
    "# Print shapes\n",
    "print(\"Source data shape:\", source_data.shape)\n",
    "print(\"Target data shape:\", target_data.shape)\n",
    "\n",
    "print(source_data)\n",
    "print(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "transformer = Transformer( tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(source_data)\n",
    "    \n",
    "    # Adjust target_data to have the same length as model output\n",
    "    target_data_adjusted = target_data[:, :output.size(1)]\n",
    "    \n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), target_data_adjusted.contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "def word_to_token_id(word, sp_model):\n",
    "    # Convert word to token ID using SentencePiece\n",
    "    return sp_model.piece_to_id(word)\n",
    "\n",
    "def generate_text(model, sp_model, starting_word, ending_word, max_length=50):\n",
    "    # Convert starting and ending words to token IDs\n",
    "    starting_token_id = word_to_token_id(starting_word, sp_model)\n",
    "    if starting_token_id is None:\n",
    "        raise ValueError(f\"Starting word '{starting_word}' not found in vocabulary.\")\n",
    "    ending_token_id = word_to_token_id(ending_word, sp_model)\n",
    "    if ending_token_id is None:\n",
    "        raise ValueError(f\"Ending word '{ending_word}' not found in vocabulary.\")\n",
    "    \n",
    "    generated_sequence = [starting_token_id]\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            input_tensor = torch.tensor([generated_sequence])\n",
    "            output = model(input_tensor)\n",
    "            predicted_token = output.argmax(-1)[:,-1].item()\n",
    "            generated_sequence.append(predicted_token)\n",
    "            if predicted_token == ending_token_id:\n",
    "                break\n",
    "    \n",
    "    # Convert token IDs to words using SentencePiece\n",
    "    generated_text = sp_model.decode_ids(generated_sequence)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "starting_word = \"needle in\"\n",
    "ending_word = \"</sos>\"\n",
    "generated_sequence = generate_text(transformer, sp, starting_word, ending_word)\n",
    "print(\"Generated sequence:\", generated_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
